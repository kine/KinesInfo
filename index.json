[{
    "title": "Using BCLicese file from powershell",
    "date": "",
    "description": "",
    "body": "New License format .bclicense If you are working with OnPrem customers having Microsoft Dynamics 365 Business Central, you have already noticed the new license file format .bclicense. It was created by Microsoft to have modern way how to pass the licensing info without limits of the old .flf format (limited max size etc.). It had some side-effect (bugs) on beginning but I hope that they are solved (never tested yet). But today I am not writing about this file format because Customers, but because it could be handy for Partners/Developers and their CI/CD pipelines.\nWhy developers should look at .bclicense file Because you need to test that all your objects in your app are assigned in the Customer\u0026rsquo;s license. Because world is not ideal, still there are plenty of customers using OnPrem for running their BC environments. And it means you need to assign object IDs into the license when you are developing some PTE. Forgetting to do so is one of the top reasons of problems during or after releasing new version of PTE into customer\u0026rsquo;s environment.\nOne way is to use the customer\u0026rsquo;s license during running your Automated Tests. You can do that easily by importing the customers license into your container/environment and run the tests. But if you want to fail fast if something is missing, and you want to be sure that everything is ok, it will not be enough. Waiting for the container to be created and the test finish will take time, and if there is not enough code covered by tests, you can still have problems. But it is still good to use the customer\u0026rsquo;s license for the tests, because you can catch the missing object permissions e.g. when doing something with tables which are limited in access by default (ledger entries etc.).\nHow to use .bclicense to check permissions If you take look at .bclicense file, you will see that it have text header as old .flf file, and rest is XML. We are interested in the XML itself, because it keeps all the info we need to work with. If we will be able to read it and work with it as with xml document, we can go through the permissions and compare them e.g. with objects we have in our AL app. But how we can access the xml part of the file in powershell? It is not so hard. Just use this PowerShell script:\n$filecontent = get-content -Path my.bclicense [xml]$xml=($filecontent[$filecontent.count-1]).Remove(0,2) After that, we have the license as XML in our $xml variable. The expression will take last line from the file (the whole XML is one-liner at the end) and remove the beginning two characters (2x U+feff).\nAnd now you can do just the standard XML tricks with the variable to check what you want. The structure looks like this:\n\u0026lt;License\u0026gt;  \u0026lt;Properties\u0026gt;  \u0026lt;Property name=\u0026#34;xxx\u0026#34; type=\u0026#34;yyy\u0026#34; id=\u0026#34;nnn\u0026#34;\u0026gt;\u0026lt;![CDATA[datadatadata]]\u0026gt;\u0026lt;/Property\u0026gt;  ...  \u0026lt;/Properties\u0026gt;  \u0026lt;PermissionCollections\u0026gt;  \u0026lt;pl t=\u0026#34;ObjectType\u0026#34; c=\u0026#34;count\u0026#34;\u0026gt;  \u0026lt;ps\u0026gt;  \u0026lt;p f=\u0026#34;from\u0026#34; t=\u0026#34;to\u0026#34; pbm=\u0026#34;PermissionMask\u0026#34; /\u0026gt;  ...  \u0026lt;/ps\u0026gt;  \u0026lt;/pl\u0026gt;\u0026gt;  ...  \u0026lt;/PermissionCollections\u0026gt;  \u0026lt;Signature\u0026gt;  ...digital signature of the file...  \u0026lt;/Signature\u0026gt; \u0026lt;/License\u0026gt; As you can see, it is simple XML. We are interested in the node /License/PermissionCollections. There we can find node for each license Object Type (TableDescription, TableData, System, MenuSuite, Codeunit, Page, FieldNo, Report, Query, XMLPort, Dataport, Form, LimitedUsageTable). In the ps (permission set?) node we can find the p node (permission?) for all the ranges in your license. It includes even ranges which you do not have in your license assigned, because such a ranges could have still some permissions in the license assigned, like \u0026ldquo;-MD-\u0026rdquo; permissions to be able to remove old unnecessary objects.\nPermissionMask value meaning Based on analysis of the file I found this bitmap mask for the PermissionMask value:\nd mirX DMIR Lowest bit of the value is on the right. It means the R permission is assigned if the value is odd and is not assigned if value is even.\nExamples:\nd mirX DMIR\r------------\r0 0000 1000 - ---D- - 8\r0 0000 1100 - --MD- - 12\r0 0000 1111 - RIMD- - 15\r0 0001 0000 - ----X - 16\r0 0001 0001 - R---X - 17\r0 0001 0010 - -I--X - 18\r0 0001 1000 - ---DX - 24\r0 0001 1001 - R--DX - 25\r0 0001 1010 - -I-DX - 26\r0 0001 1101 - R-MDX - 29\r0 0001 1111 - RIMDX - 31\r0 1100 0001 - Rim-- - 193\r0 1100 1001 - RimD- - 201\r1 0100 0101 - RiMd- - 325\r1 1000 0011 - RImd- - 387\r1 1100 0001 - Rimd- - 449\r1 1110 0000 - rimd- - 480 Thus you need to be aware that it is not enough to test if the ID of your object is included in some range, but you need to check if there is X permission to execute it on the range, M and I permission to be able to modify or create the object by publishing standard APP package (if these permissions are missing, you can only publish such an object by Runtime Package) etc.\nYou can test the permission e.g. by this function:\nfunction Check-PermissionMask {  param(  [Parameter(Mandatory=$true,  ValueFromPipeline=$true,  ValueFromPipelineByPropertyName=$true,  HelpMessage=\u0026#34;Permission value\u0026#34;)]  [int]$PermValue,  [Parameter(Mandatory=$true,  ValueFromPipeline=$true,  ValueFromPipelineByPropertyName=$true,  HelpMessage=\u0026#34;Permission to test\u0026#34;)]  [validateset(\u0026#39;r\u0026#39;,\u0026#39;m\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;X\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;D\u0026#39;)]  [string]$PermToTest  )   $PermMask = \u0026#39;RIMDXrimd\u0026#39;  $TestValue = 1 -shl $PermMask.IndexOf($PermToTest)  Return [bool]($PermValue -band $TestValue) }  # Test of the function: Check-PermissionMask -PermValue 15 -PermToTest \u0026#39;R\u0026#39; True CI/CD process Thanks to the .bclicense you can extend your pipelines to test if license was correctly extended with new objects and you do not need to wait for the container (and fail as early as possible). You still need to solve one thing which is out of scope of this post - how to get the list of objects in your app without creating the container and publishing the app first. I am sure there will be some way (like going through all .al files and do some basic text parsing to find all the object headers, but it still could be complex e.g. to cover correctly temporary tables etc.). If you have some tip regarding that, do not hesitate to share it in comments. If I will create something regarding that, I will try to share it.\nSummary You can use the new .bclicense format to work with the data inside the license easily. But you need to solve few things first, like where the actual license for the customer will be stored, how to get the list of objects in app etc. Use the knowledge as you need, I hope that this article will save you some time you will need to spend on analysis of the file yourself. I am sure that there will be plenty things you can thing out around this.\n",
    "ref": "/posts/bclicensestructure/"
  },{
    "title": "How to translate your AL app from Devglish to Endglish",
    "date": "",
    "description": "",
    "body": "Since we started to use VSCode for AL development, I hear discussions that writing UI texts like ToolTips etc. should be done by someone else than developer. And I must agree. We - developers - are lazy kind and writing text is not our popular game. We are able to write some generic text, mostly automagically generated by different tools we are using to write code, but such a text is mostly equal to 0 Shannons (no, I am not talking about @Shannon Mullins). But how to solve this?\n  Text written by developer  Ask Microsoft to solve it On many occasions was this subject mentioned and discussed with Microsoft product group. I am sure that they are aware of this, but the list of To-Do is too long and nobody knows where in the list is this to solve. And I understand that, because there are things which are more important. But may be, once we will get something which will solve that.\nDo you know the \u0026ldquo;Blend for Visual Studio\u0026rdquo;? It is tool for designers to create the design of the app, without solving the code behind. Visual Studio is used for writing the code, and let the design be done in the Blend. Of course, we do not need such a huge tool.\nAnother example how these things are solved in other applications are the \u0026ldquo;placeholders\u0026rdquo; (X++ in Dynamics 365 AX was using this if I remember correctly and I assume that the successors are still using it). Developer just use some kind of code instead text and the text is assigned to the code somewhere outside the code. But it is much harder to understand the code, when you see only something like Message(\u0026quot;@SYS654433\u0026quot;).\nUse what we have But today I had discussion with my colleague Martin, and we were thinking how to put our consultants in charge of the content of our ToolTips etc. Of course, one way was to give them access to the code and let them overwrite the texts in the code. And I do not like it (of course, they will need e.g. to have license for Azure DevOps to be able to work with the repos etc.). But then I realized, that there is already one way, which our consultants are using to manage the texts - translations. They have possibility to do the translations to different languages as needed, because the XLIFF files for our apps are available and could be updated and then developers can incorporate the translations into new versions of the apps. It means, we already have everything what we need.\nHow? Just create new translation file for en-us language (there are multiple vscode extensions which could help you with the xliff files management - e.g. XLIFF Sync or NAB AL Tool) and do the ENU to ENU translation. This time it will not be about translating the text as it is to different language, but rather create new, better text describing all what end-user needs to see. In this way, developers could put \u0026ldquo;placeholders\u0026rdquo; into the code, e.g. just simple description what is going on, which text it is etc. (like \u0026ldquo;Tooltip for field My New Shiny Value\u0026rdquo;) or just use the generic values as you are used to. No problem. And then, let the consultants translate these \u0026ldquo;technical\u0026rdquo; texts into the real helpful end-user-centric texts. I am sure, that you already have processes which covers translation of your apps. And if something will not be \u0026ldquo;translated\u0026rdquo;, users will see the standard text from developers. And consultant could search for this text in the en-us translation file and change it as needed, you do not need to fix that in code (if it is not issue of fact etc.). Just update the translations, publish new app version and you are done!\nConclusion I see as best practice to have en-us XLIFF file in each app to solve this translation from \u0026ldquo;Devglish\u0026rdquo; (Developer\u0026rsquo;s English) to \u0026ldquo;Endglish\u0026rdquo; (End-user English). Let developers write the text in code in Devglish, and let consultants add the Shannons to the texts to transfer information to the Users and create the new Endglish layer.\n  Translation from Devglish to Endglish  Downside There is one downside of this: your translation sources for other languages will be based on the Devglish version, not the Endglish version of the EN texts. But it is solvable and we will see, if some vscode extension will add functionality allowing to update the translation files from .en-US. version instead .g. version of the xliff. It means having the update chain changed from\n\rflowchart LR\r.g. -- .cs-CZ.\r.g. -- .sk-SK.\r to this flowchart LR\r.g. -- .en-US.\r.en-US. -- .cs-CZ.\r.en-US. -- .sk-SK.\r It will allow us even to update the translation in other languages when someone change the Endglish version and thus keep in sync the languages. I believe in BC community in this!\nP.S.: Do not put the Endglish translations into the .g. file directly, they will be lost when you compile the app, because .g. file is generated file, it is not source file. This is why it should not be part of the source code repository (add it into .gitignore file).\nFeedback If you have anything to add, comment or you want to share your way of solving these things, do not hesitate and put your comments here! I am looking forward the feedback and I am open to discuss this!\nThanks!\n",
    "ref": "/posts/whytowriteenutranslation/"
  },{
    "title": "Unattended access to local resources - WCF Relay",
    "date": "",
    "description": "",
    "body": "In many cases you need to work with resources (files, scales, card readers etc.), which are available on local network/PC, from within Microsoft Dynamics 365 Business Central which runs somewhere in the Cloud. I will show you one way how to solve these things.\nWhat is the problem? Different processes in Business Central can need some local resources of yours. In a simples case it could be some file on your disk on your PC (or server on local network). Standard way is, that BC client will ask user to select the file which should be uploaded or select where to save downloaded file. It needs user interaction. But many automatic processes needs to run without this user interaction. E.g. to monitor some local folder for new files, read them, process them and move them to archive. You can use some Azure Storage to be able to do that in compatible way, but in some cases it is not possible, for example when the file is generated by some ancient technology or some local hardware. Still you have possibility to \u0026ldquo;synchronize\u0026rdquo; these local files to the cloud storage by some technology, but it have own problems (we were using OneDrive synchronization for this, but the reliability was very low, mainly because we needed pseudo-online updates).\nHybrid solution To solve this problem, we can use some local running component, which have access to the local resource, and somehow connect it with the Cloud. \u0026ldquo;Standard\u0026rdquo; solution will be to have service, which offer some API. But because you need to connect from Cloud (from internet), you will need to somehow tunnel the outside incoming traffic to the local service. It will mean to create \u0026ldquo;hole\u0026rdquo; in your firewall to allow this. To limit possible security issues you can try to limit this hole for specific IPs from which the traffic will be allowed, but it is hard to do that for Business Central, because I do not know any specific IPs which are used for the BC services.   Hybrid connection diagram  Luckily we have different Azure Services, and one of them is solving this issue - Service Bus (specifically WCF Relay). This service is allowing your service to work in \u0026ldquo;opposite\u0026rdquo; direction - your service is connecting to the cloud from inside your network. It means you only need to allow outgoing connection in your local network which is much easier (and mostly is not limited, because it is standard connection to https endpoint on internet). Azure Relay will create the public endpoint on which you can consume the API you defined in your locally running service. To implement this you need only few settings in the app.\nOf course, it have some cost. You are paying some small amounts per each \u0026ldquo;relay hour\u0026rdquo; - it means for each hour your service is running and is connected to the Azure Relay (I can see standard cost 0.0090 EUR per 100 relay hours and 0.009 EUR per 10 000 messages).\nImplementation To implement this solution, you need to follow these steps:\n Implement Windows Service with Rest API with local endpoint Implement WCF Relay to have public endpoint Implement BC connector for this service  Windows Service with Rest API How it could look like you can see here: LocalFS Service\nI am not expert in c#, thus take it as a work of beginner based on examples found on internet (like this).\nThis windows service will open one local endpoint on port selected in the config. Second endpoint will be created through Azure WCF Relay.\nService have 3 main areas:\n REST API (contract and implementation of the API) Windows service handling (main program and actions to register/start/stop the service) Using the WCF Relay to create public endpoint  WCF Relay To be able to connect to the locally running service from outside your local network, you need to connect it to Azure Service Bus. To be able to do this, you need to create the Relay namespace in Azure portal.   Creating Relay namespace  The namespace is creating the URL which is unique for your relay. The public endpoint you will need to connect to, will have name like this:\n https://{RelayNameSpace}.servicebus.windows.net/{RelayServicePrefix}/{something}\n WCF Relays could be of two types: Dynamics or Static. Static can be created on the portal, but we will rather use the dynamic ones. They are created and removed automatically when you start/stop your service. RelayServicePrefix in the URL is any string you will choose. In this way you can group multiple different services together. The last part of the URL is again text you choose to make the URL unique. In my example code I am using computer name and domain name to create this part (like \u0026ldquo;PC0123.mydomain.local or \u0026ldquo;PC0123.\u0026rdquo; if there is no domain).\nTo be able to register the service into the relay, you need to use Shared access policy key. You can get it on the Azure portal in the Shared access policies section.   Getting SAS key  When you set the values and run the service, you should see the service in the WCF Relay list on the Azure portal.\nBusiness Central connector To connect to the service from Business Central, use standard HTTPClient to connect to the relay address. It is standard REST API, thus it should not be a problem for you to do that. If you want to use this LocalFS service, you can use our AppSource app Navertica Local FS Connector.\nI am sure that you will find many examples how to implement REST API client in AL on internet.\nSecurity The WCF Relay is transparent and it means it is on your service to make the authentication of the caller. In the example I am using just basic mechanism which checks that the used user name and password is same as configured one.\nI am sure that there is some possibility to add security on the WCF Relay, but have not time to dig into it yet. If you have some tips, you can use the comments to share them.\nConclusion I wanted to show you possible way how to handle things which were blockers some time ago for using Business Central Online. You can easily modify the service to not handle local files access but e.g. communicate with some locally connected hardware (special printers, scanners, scales etc.). Such a things should not prevent you now from going to cloud. It have some cost (cost of the WCF Relay), but I think it is really small cost which is overweighed by what you will get.\nI know that there are other ways how you can solve such a cases, but this one is for me easy to use even with my limited knowledge. I hope it will help you to think outside the box and unblock your way.\nSee you in the Cloud!\n",
    "ref": "/posts/localfs/"
  },{
    "title": "Snapshot debugger on OnPrem - what you need to know to set it up",
    "date": "",
    "description": "",
    "body": "When you want to use Snapshot Debugging on your OnPrem environment, may be you can hit this error when trying to initialize it:\nError: The SSL connection could not be established, see inner exception.\r... In some cirumstances you can even have problem to start the BC Service when you enable the Snapshot Debugger endpoint.\nWhat is the problem? For standard endpoint, which we are using for longer time already, like client endpoint (default port 7046), OData (7048) and Dev (7049), the Management console for BC Server is automatically setting some things in background when you save the settings in it. But for the Snapshot debugger, this is not done and you need to do it yourselfs.\nURLACL First thing is, that for the user account, under which is BC Server running, usage of the HTTP address is reserved (see this). Manually it could be done by running this command as admin on the server:\nnetsh http add urlacl url=https://+:7083/BC/ user=\u0026#39;NT AUTHORITY\\NETWORK SERVICE\u0026#39; You need to modify the URL if you are not using SSL (use http instead https) and port number/instance name. User needs to be the one under which the service is running.\nTo list existing urlacl you can use this command:\nnetsh http show urlacl If this is not done, you will ger some error about \u0026ldquo;unable to listen\u0026rdquo; on the specified address in your event log.\nSSLCERT Second step is to assign certificate to the address, if you are using SSL for the endpoint.\nTo use SSL it is not enough to set the certificate in the BC Service Management console (in the BC Service configuration), but the certificate must be assigned to the address.\nThis is done again through netsh command:\nnetsh http add sslcert certhash=\u0026lt;thumbprint\u0026gt; appid=\u0026#39;{00112233-4455-6677-8899-AABBCCDDEEFF}\u0026#39; ipport=0.0.0.0:7083 Use the thumbrint of the certificate you want to use (e.g. same like in the BC Service configuration), appid could be any GUID (e.g. like in th example), and ipport must be the port for which you are assigning the certificate.\nTo list existing assignments you can use this command:\nnetsh http show sslcert If you want to assign certificate to port, where already is certificate assigned (e.g. when renewing expiring certificate), you need to delete the sslcert first:\nnetsh http del sslcert ipport=0.0.0.0:7083 Some hicups Sometimes, when you are changing the SSL settings on the BC server, you can get into issues that when saving the configuration you get some errors that Certificate cannot be registered or something similar. It is mostly in situations, when the settings and the current state of the SSLCERT is not in line - it means, you are disabling SSL but SSLCERT is not registered (e.g. after you have copied configuration from another instance having SSL enabled and you current instance had it disabled) or vice versa. In this case you need to fix this discrepancy (when disabling SSL, create the SSLCERT assignment first, when enabling, remove the existing assignemnt first).\n",
    "ref": "/posts/snapshotdebuggeronprem/"
  },{
    "title": "Architecture of PTE - Split or not to Split?",
    "date": "",
    "description": "",
    "body": "On Directions EMEA in Milan I delivered session about Architecture of PTE for Business Central. I was trying to find some rules when or why you should or shouldn\u0026rsquo;t split your PTE into multiple apps. Now I will try to catch this into this article.\nSplit or not to split Some partners are putting everything into one app per customer. Some are splitting all to separate apps based on different rules like per process, per area etc. Our company is going through the way of splitting, rather than not splitting, because on the beginning we wanted to have possibility to re-use the \u0026ldquo;tiny apps\u0026rdquo; for others if needed. But the result? Here is the picture (taken from real live customer project):\n  Real world apps dependencies example  As you can see, some apps are \u0026ldquo;self standing\u0026rdquo;. This is what we wanted to have. But most of them are connected with others. That\u0026rsquo;s something we didn\u0026rsquo;t want. What does it mean? They are not re-usable, if you do not want to re-use even the dependencies. It means you can reuse only the \u0026ldquo;leftmost\u0026rdquo; apps. It makes most of the apps non-reusable without refactoring.\nBut the question stays: Split or not to split?\nTypes of PTE apps based on the chart, I made some analysis and found these sets of apps based on names of the apps:\n TOOLS/LIBRARIES - \u0026ldquo;Record links\u0026rdquo;, \u0026ldquo;Control Focus\u0026rdquo;, \u0026ldquo;Barcodes\u0026rdquo;, \u0026ldquo;PDF Signature\u0026rdquo; IMPORTS/EXPORTS - \u0026ldquo;Data Migration\u0026rdquo;, \u0026ldquo;Salary Import Connector\u0026rdquo;, \u0026ldquo;Banking\u0026rdquo; CONNECTORS - \u0026ldquo;Manufacturing API\u0026rdquo;, \u0026ldquo;Laboratory Integration\u0026rdquo;, \u0026ldquo;NAV Web Service Adapter\u0026rdquo; SIMPLE PROCESSES - \u0026ldquo;VAT Posting Date Change\u0026rdquo;, \u0026ldquo;Standard Cost Management\u0026rdquo;, \u0026ldquo;Approval\u0026rdquo;, \u0026ldquo;On Hold\u0026rdquo; COMPLEX PROCESSES OR AREAS - \u0026ldquo;Service Acceptance\u0026rdquo;, \u0026ldquo;Spare Parts\u0026rdquo;, \u0026ldquo;Overhead Material\u0026rdquo;, \u0026ldquo;FA Inventory\u0026rdquo;, \u0026ldquo;Price Security\u0026rdquo;, \u0026ldquo;Manufacturing\u0026rdquo;, \u0026ldquo;Report Pack\u0026rdquo;… EXTENSIONS (CUSTOMIZATIONS) - \u0026ldquo;Handling Unit Extension\u0026rdquo;, \u0026ldquo;Continia Extension\u0026rdquo;, \u0026ldquo;Small modifications\u0026rdquo;, „Intrastat Extension\u0026quot;  We can define some properties of each type, which could help us to identify them:\n TOOLS/LIBRARIES - Adding functionality which is used through whole system, independent on specific processes/areas IMPORTS/EXPORTS - working with files or other data sources CONNECTORS - connecting with other systems (through API - as client or as server) or between extensions SIMPLE PROCESSES - mostly adding some small tasks and processes, limited schema changes (not many fields and tables) COMPLEX PROCESSES or AREAS - creating new processes or whole areas, new tables, pages… EXTENSIONS (CUSTOMIZATIONS) - are modifying app which is in responsibility of someone else (3rd part, different product group etc.)  And based on their properties, we can divide them into two groups:\n Not Problematic  TOOLS - mostly depending on standard only (lowest level dependencies) IMPORTS/EXPORTS - low probability of depending apps, rather depends on other (similar to CONNECTORS), sometimes need to be OnPrem if we are OnPrem CONNECTORS - should be top (or bottom) app in the tree   Problematic  SIMPLE PROCESSES - you do not know if it will not grow to be complex COMPLEX PROCESSES or AREAS - hard to split the processes correctly and solve the dependencies when cyclic EXTENSIONS (CUSTOMIZATIONS) - could have depending apps, could grow to complex processes    But still: Split or not to split?\nDifferences Ok, previous classification is not helping us to answer the question. We should look at the CONs and PROs of the splitting the PTE into separate Apps:\nSplitting  PRO:  Easier parallel development (development split to more apps) Clear responsibilities Simpler/Smaller apps Simpler maintenance Re-use (really?) List of functionalities Possibility to „uninstall\u0026quot; customization when not needed (really?)   CONS:  Dependency „hell\u0026quot; (solvable) Architecture decisions are more complex Performance (solvable) Release complexity (solvable) Cost of maintenance per app (partially solvable)    We can discuss if the apps are re-usable (we already touched this). But most of the CONs are solvable by using correct tools (partial records etc.) and automatic processes (release pipelines, powershell etc.).\nAll-in-one  PRO:  Simplified Architecture Simplified release to Customer Cost of maintenance per app Performance (! not because technical issues, but because mindset of the developers - \u0026ldquo;it is in one app, why bother with performance?\u0026rdquo;)   CONS:  Developers conflicts (solvable) Release Planning (solvable) Installation times (? in history, deploying big app took more time) Impossible to re-use without refactoring What is inside (solvable) Shared responsibilities    And again, you can see that most of the CONs could be solved by tools (AL Object ID Ninja, documentation etc.) and processes (Azure DevOps planning etc.)\nStill: Split or not to split?\nComparing Ok, compare the PROs and CONs of both methods when we remove the solvable items:\n  PROs and CONs comparison  Items I think are most interesting are in bold.\nAll depends on priorities you have in your company or team. If your priority is e.g. \u0026ldquo;Simpler maintenance\u0026rdquo; and \u0026ldquo;Clear responsibility\u0026rdquo; and you sacrifice the \u0026ldquo;easy architecture decisions\u0026rdquo;, go and split the apps.\nIf your priority is simpler \u0026ldquo;architecture decisions\u0026rdquo; and you sacrifice \u0026ldquo;shared responsibility\u0026rdquo; (who is responsible for the content of big, all-in-one app?), go and do not split.\nOk, does it mean split or not to split?\nSMART Architecture For me, both ways are valid. But you need to do it SMART.\n SIMPLE MANAGABLE ARCHITECTURE REACHING TARGET  And to do it, you need to know what the target is for you, your team, your company, your customers.\nWhat it could mean? For example this:\n Keep TOOL/LIBRARIES and CONNECTOR apps separate Use Dependency Inversion principles (interfaces etc.) where you want to \u0026ldquo;reverse\u0026rdquo; the dependency Try to keep the responsibilities clear. Boundary of App is boundary of responsibility. Public interface of App is contract between apps. When you need to depend on new App (3rd party etc.), try to create CONNECTOR as separate app to prevent adding the dependency into the \u0026ldquo;core\u0026rdquo; app if possible. When you need to switch the app to „OnPrem\u0026quot;, create CONNECTOR (working with local files etc.) - leave rest of the logic in „Cloud\u0026quot; (and best is to not use OnPrem at all - it will cost you, literally) Content of the app must be in line with app name. If the change is not in line with the app name, may be it should be somewhere else. Choose wise the name of the app.  Conclusion May be you have still questions, what is THE BEST way. But I think there is no silver bullet for this. The answer could be different, depending on who you ask. But if you already go some way, do not switch just because someone tells you that it is better in another way. May be he/she is solving different problems than you and your solution in your situation is the best for you.\nI hope that this article helps someone to think about the architecture of Business Central PTEs from different angles, and I am open to discussion about the architecture at all. What I see now as biggest gap in our community, is lack of architects and discussion about the architecture itself. And this my session and article is for me starter for the discussion we will have next years.\nBe safe! See you online/onprem!\n",
    "ref": "/posts/ptearchitecture1/"
  },{
    "title": "My new blog",
    "date": "",
    "description": "",
    "body": "New blog May be you already noticed that my original blog on dynamicsusers.net (original URL was https://www.dynamicsuser.net/nav/b/kine) is gone. I am building new one from scratch. May be it is time to begin something new and cut off the Dynamics NAV history.\nBecause todays it is easy to use GitHub as a platform for the blog (and thus using git and vscode to \u0026ldquo;post\u0026rdquo; the articles), I decided to use HUGO as the framework. Another side of this is, that my blog is \u0026ldquo;opensourced\u0026rdquo;, thus you can see everything what is there and if you have some need to fix something, you can easilly post pullrequest.\nI hope that you will find it worth to read it.\nEnjoy it!\n",
    "ref": "/posts/newblog/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "About me I have more than 35 years of experience with software development and IT in general. In the year 2001, after finishing university, I joined the NAV world in NAVERTICA company as NAV developer. Having a wide area of knowledge in programming languages and connected areas is helping me to understand “how it works” and is giving me background for solving different tasks. For the last years, I am working more with GIT, Powershell, and Azure DevOps. I am a Microsoft Most Valuable Professional (MVP) since the year 2005.\nWhat I know I have learned mostly on my own (many trials\u0026amp;errors), because I started when there was no internet and even no books about computers. Our country lived in communism and my first computer Atari 800XE was bought in Austria (thanks to my parents for this decision). But all that gave me something, what cannot be taught.\nIn personal life I have beloved wife and my free time activity is to play PC games/simulators like IL-2 Sturmovik, MS Flight Simulator and other.\n",
    "ref": "/about/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "If you need professional services from me, contact NAVERTICA a.s. sales. I am employee in the company and I am not able to serve you directly.\nIf you want to support me personally, you can use the BuyMeCoffee tipping system.\n",
    "ref": "/contact/"
  }]
