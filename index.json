[{
    "title": "SaLi architecture for Microsoft Dynamics 365 Business Central, part 1",
    "date": "",
    "description": "",
    "body": "If you are developing solutions for Microsoft Dynamics 365 Business Central and you are building them from multiple applications (see Split or not to split), may be you are now experiencing some problems with the dependencies between your apps, chaos made by development withouth rules. We were there too and this is example of dependency chart of one our solution: Dependencies before SaLi And because my coleague Jakub Linhart is smart and tried to analyze our existing solutions to find some ways how to make them and our new solutions better, we have noticed some patterns and we tried to make some rules based on these patterns. Then I read book \u0026ldquo;Clean Architecture\u0026rdquo; from Robert C. Martin and learned about SOLID principles. And all started to give some meaning. And SaLi was born.\nSaLi Why SaLi? Just simple because Sacek and Linhart. But WHY SaLi? Because doing good architecture without rules is like trying to build high tower from bricks just by throwing the bricks on one heap. Bricks on the heap And SaLi is giving us some rules we can follow, to create nice solution which will be SOLID. Bricks with some rules Application Groups First thing we have noticed was, that applications we have developed, could be divided into multiple groups based on their purpose. This groupping was already mentioned in Split or not to split article. But we made some fine-tuining on them and weare now using these application groups:\nShared/Discrete apps Apps providing discrete functionality or adding totally new area of functionality. These apps can live alone (or on top of \u0026ldquo;standard\u0026rdquo; functionality) and will provide functionality to the end-user which could be used directly (Discrete functionality). Another attribute of these app is, that they have data which are used by many other applications in the solution (Shared). In these apps you have mostly some Master tables, Journal tables, Ledger tables, Document tables etc.\nProcess/Automations apps Apps of this type are interconnecting other apps, adding some automation on top of existing functionality. They are mostly not adding new processes or functionality, they are simplifying existing functionality or are \u0026ldquo;glueing\u0026rdquo; functionality of multiple separate apps. They have no meaning withouth the apps which they are extending and are implementing different processes of the company.\nInput/Output apps These apps are defining outer interface of the system. Are implementing APIs (input/output for other systems), changing GUI (output to display), adding Reports (output to printer/display). They should not have functionality which is not directly connected with the interface they are connecting. They are interface-specific (implementing specific interface connection, API vendor etc.).\nLayers In next step, after defining application groups, we have found that these apps should create layers, which are defined by their dependencies. Together with SOLID principles, we can assign different attributes to the groups like:\nShared/Discrete apps are defining critical business processes, are more abstract/universal, are stable (not changed too often), depending on less apps, more apps are depending on them Process/Automation apps are defining processes, are not stable as Shared/Discrete apps, could depends on more apps than depending on them Input/Output apps are unstable because are changed often, are depending on specific interfaces (are specific), are depending on other apps, no apps are depending on them And this created layers which could look like this: SaLi layers as circle Dependencies Rule Based on SOLID principles, the dependencies could lead only from outer layer to inner layer, never oposite, because unstable apps should depends only on more stable apps. Specific apps should depends on generic apps, not critical apps should depends on more business critical apps etc.\nThis gives us first rule of SaLi: \u0026ldquo;Dependency direction is given by layers into which the apps belongs. Dependency could go only from outer layer to inner layer (or from top to bottom if, see later). If needed in oposite, use the Dependency Inversion Principle to switch the dependency direction\u0026rdquo;.\nAnother way how to describe the layers is like this: SaLi layers You can see that we have even the AppSource app layer and Core app layer. These are apps out of our control and we can only depend on them. But even when you are developing solution for AppSource, the layers Shared/Process/Input\u0026amp;Output are about your AppSource solution (if it is created from multiple apps) and the AppSource layer means other AppSource apps you can be depending on.\nThe arrows are marking the direction in which the dependencies could be created in between apps.\nApplying this rule, and use the layers when creating application dependency chart, will result in something like this: Dependencies after SaLi But still, on this picture you can see arrows which are breaking the rule (connecting e.g. apps on same layer, going from bottom up etc.). Yes, these are errors in the architecture, and SaLi is helping us to see them. When we see them, we can fix them by implementing Dependency Inversion Principle (using interfaces, events\u0026hellip;). But without SaLi they are nicely hidden and will pop up in the least appropriate time.\nAnother income of SaLi in this case is, that we have the layer assigned to the app stored in our metadata for every app (we are using Work Items in Azure DevOps for this) to be able to create the chart. And because this, we can implement check in DevOps pipelines which can validate if the dependencies are in correct directions and can enforce developers to create them correctly (work in progress on this).\nNext In next article about SaLi we will look on other rules we are using.\n",
    "ref": "/posts/sali_part1/"
  },{
    "title": "How to publish your local BC containers to internet using IIS",
    "date": "",
    "description": "",
    "body": "If you are doing Dynamics 365 Business Central development on your local (not Azure) containers, may be you want to have access to them from outside your local network without using VPN. And not only to BC Web Client, but even to API/OData and development endpoint. And of course you want to have them published with some trusted certificate to be able to use all the functionality like Business Central application, connection from Power BI connectors etc. It is handy when you are working with external workers like freelancers, because you do not need to setup VPN etc. for them. When using AAD authentication, just invite them as guest users to your tenant and you are ready to give them access as needed.\nDifferent tools exists for solving that like Traefik.\nBut if you want just easy solution using Windows IIS, you can use the Application Request Routing (ARR) feature of IIS for that.\nHow it works When we create the proxy server (server with Windows and IIS installed) and we put it somewhere into our DMZ (zone which is accessible from internet and can access local network), we can setup DNS server to send our requests for some specified subdomain with our \u0026ldquo;BC Environments\u0026rdquo; to that proxy. The proxy takes the request and will forward it to correct internal server/container and will transfer the response back to the sender. In my case, all our internal development environments are running as part of our dev. subdomain, thus I am able to forward everything going to xxx.dev.ourdomain.com to our local xxx.dev.ourdomain.com.\nProxy architecture And if I set the proxy to listen on port 443, 7047, 7048, 7049 and 7085, I will make the web client, SOAP, OData/API, Development and Snapshot debugging available for people from outside.\nNeeded components External DNS Entry On your external DNS for your dev.ourdomain.com zone, you need to create wildcard entry of type CNAME leading to the external interface of your PROXY. It will translate anything in format xxx.dev.ourdomain.com to the proxy server address.\nInternal DNS Entries On your internal DNS you need to translate same address as you want to use from outside, to local address of the server/container. It means, on your internal network the address xxx.dev.ourdomain.com should be translated to the local address of the server/container. In this way we do not need any \u0026ldquo;translation\u0026rdquo; table which will transform the external name to internal one and we can just maintain the local DNS entries. Having same external and internal address is even good thing when you want to use AAD authentication in BC.\nForward Proxy On the proxy, install IIS feature with ARR and URL Rewrite. You can use this documentation to install and setup what is needed. Enable the SSL offloading to be able to have different internal and external SSL certificate. When the offloading is enabled, the proxy will act as man-in-the-middle. It means the external client is communicating with the external https endpoint of the proxy, which have some externally trusted certificate installed on it. The proxy then takes the request and will create new connection to the local resource and send the request there. The local resource could have internally trusted certificate, or could use http protocol without certificate if you want.\nSet the rules like this:\nGlobal forward rule Proxy rule 1 Rule to handle OAuth request correctly (without this the OAuth will end on the container instead the Microsoft site) Proxy rule 2 Rule to handle OAuth request correctly (without this the OAuth will end on the container instead the Microsoft site) Proxy rule 3 Set the default web site on the IIS to listen on the required ports like 443, 7048, 7049 and 7085. Open these ports for access from internet on the firewall.\nConclusion If everything is working correctly, you should be able:\nconnect to the xxx.ourdomain.com web client and other endpoints from local network connect to the xxx.ourdomain.com web client and other endpoints from internet without using VPN have trusted certificate placed only on the proxy server on the default site have internal certificate on your environments (proxy server must trust them) If you are missing some info here, let me know and I will update the article as needed.\nWarning! This setup is forwarding everything going to the *.dev.ourdomain.com to the proxy, thus it could be used for attack to your internal infrastructure, if you have more services available than just the BC containers on your internal dev.ourdomain.com domain. You need to be aware of that and you can somehow limit this by modifying the rules e.g. to forward only request having format of the names of your BC environments etc. Best is to use subdomain where is nothing else than the environments.\nEnjoy work with the environments withouth VPN!\n",
    "ref": "/posts/proxy/"
  },{
    "title": "Using BCLicense file from powershell",
    "date": "",
    "description": "",
    "body": "New License format .bclicense If you are working with OnPrem customers having Microsoft Dynamics 365 Business Central, you have already noticed the new license file format .bclicense. It was created by Microsoft to have modern way how to pass the licensing info without limits of the old .flf format (limited max size etc.). It had some side-effect (bugs) on beginning but I hope that they are solved (never tested yet). But today I am not writing about this file format because Customers, but because it could be handy for Partners/Developers and their CI/CD pipelines.\nWhy developers should look at .bclicense file Because you need to test that all your objects in your app are assigned in the Customer\u0026rsquo;s license. Because world is not ideal, still there are plenty of customers using OnPrem for running their BC environments. And it means you need to assign object IDs into the license when you are developing some PTE. Forgetting to do so is one of the top reasons of problems during or after releasing new version of PTE into customer\u0026rsquo;s environment.\nOne way is to use the customer\u0026rsquo;s license during running your Automated Tests. You can do that easily by importing the customers license into your container/environment and run the tests. But if you want to fail fast if something is missing, and you want to be sure that everything is ok, it will not be enough. Waiting for the container to be created and the test finish will take time, and if there is not enough code covered by tests, you can still have problems. But it is still good to use the customer\u0026rsquo;s license for the tests, because you can catch the missing object permissions e.g. when doing something with tables which are limited in access by default (ledger entries etc.).\nHow to use .bclicense to check permissions If you take look at .bclicense file, you will see that it have text header as old .flf file, and rest is XML. We are interested in the XML itself, because it keeps all the info we need to work with. If we will be able to read it and work with it as with xml document, we can go through the permissions and compare them e.g. with objects we have in our AL app. But how we can access the xml part of the file in powershell? It is not so hard. Just use this PowerShell script:\n$filecontent = get-content -Path my.bclicense [xml]$xml=($filecontent[$filecontent.count-1]).Remove(0,2) After that, we have the license as XML in our $xml variable. The expression will take last line from the file (the whole XML is one-liner at the end) and remove the beginning two characters (2x U+feff).\nAnd now you can do just the standard XML tricks with the variable to check what you want. The structure looks like this:\n\u0026lt;License\u0026gt; \u0026lt;Properties\u0026gt; \u0026lt;Property name=\u0026#34;xxx\u0026#34; type=\u0026#34;yyy\u0026#34; id=\u0026#34;nnn\u0026#34;\u0026gt;\u0026lt;![CDATA[datadatadata]]\u0026gt;\u0026lt;/Property\u0026gt; ... \u0026lt;/Properties\u0026gt; \u0026lt;PermissionCollections\u0026gt; \u0026lt;pl t=\u0026#34;ObjectType\u0026#34; c=\u0026#34;count\u0026#34;\u0026gt; \u0026lt;ps\u0026gt; \u0026lt;p f=\u0026#34;from\u0026#34; t=\u0026#34;to\u0026#34; pbm=\u0026#34;PermissionMask\u0026#34; /\u0026gt; ... \u0026lt;/ps\u0026gt; \u0026lt;/pl\u0026gt;\u0026gt; ... \u0026lt;/PermissionCollections\u0026gt; \u0026lt;Signature\u0026gt; ...digital signature of the file... \u0026lt;/Signature\u0026gt; \u0026lt;/License\u0026gt; As you can see, it is simple XML. We are interested in the node /License/PermissionCollections. There we can find node for each license Object Type (TableDescription, TableData, System, MenuSuite, Codeunit, Page, FieldNo, Report, Query, XMLPort, Dataport, Form, LimitedUsageTable). In the ps (permission set?) node we can find the p node (permission?) for all the ranges in your license. It includes even ranges which you do not have in your license assigned, because such a ranges could have still some permissions in the license assigned, like \u0026ldquo;-MD-\u0026rdquo; permissions to be able to remove old unnecessary objects.\nPermissionMask value meaning Based on analysis of the file I found this bitmap mask for the PermissionMask value:\nd mirX DMIR Lowest bit of the value is on the right. It means the R permission is assigned if the value is odd and is not assigned if value is even.\nExamples:\nd mirX DMIR\r------------\r0 0000 1000 - ---D- - 8\r0 0000 1100 - --MD- - 12\r0 0000 1111 - RIMD- - 15\r0 0001 0000 - ----X - 16\r0 0001 0001 - R---X - 17\r0 0001 0010 - -I--X - 18\r0 0001 1000 - ---DX - 24\r0 0001 1001 - R--DX - 25\r0 0001 1010 - -I-DX - 26\r0 0001 1101 - R-MDX - 29\r0 0001 1111 - RIMDX - 31\r0 1100 0001 - Rim-- - 193\r0 1100 1001 - RimD- - 201\r1 0100 0101 - RiMd- - 325\r1 1000 0011 - RImd- - 387\r1 1100 0001 - Rimd- - 449\r1 1110 0000 - rimd- - 480 Thus you need to be aware that it is not enough to test if the ID of your object is included in some range, but you need to check if there is X permission to execute it on the range, M and I permission to be able to modify or create the object by publishing standard APP package (if these permissions are missing, you can only publish such an object by Runtime Package) etc.\nYou can test the permission e.g. by this function:\nfunction Check-PermissionMask { param( [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true, HelpMessage=\u0026#34;Permission value\u0026#34;)] [int]$PermValue, [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true, HelpMessage=\u0026#34;Permission to test\u0026#34;)] [validateset(\u0026#39;r\u0026#39;,\u0026#39;m\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;X\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;D\u0026#39;)] [string]$PermToTest ) $PermMask = \u0026#39;RIMDXrimd\u0026#39; $TestValue = 1 -shl $PermMask.IndexOf($PermToTest) Return [bool]($PermValue -band $TestValue) } # Test of the function: Check-PermissionMask -PermValue 15 -PermToTest \u0026#39;R\u0026#39; True CI/CD process Thanks to the .bclicense you can extend your pipelines to test if license was correctly extended with new objects and you do not need to wait for the container (and fail as early as possible). You still need to solve one thing which is out of scope of this post - how to get the list of objects in your app without creating the container and publishing the app first. I am sure there will be some way (like going through all .al files and do some basic text parsing to find all the object headers, but it still could be complex e.g. to cover correctly temporary tables etc.). If you have some tip regarding that, do not hesitate to share it in comments. If I will create something regarding that, I will try to share it.\nSummary You can use the new .bclicense format to work with the data inside the license easily. But you need to solve few things first, like where the actual license for the customer will be stored, how to get the list of objects in app etc. Use the knowledge as you need, I hope that this article will save you some time you will need to spend on analysis of the file yourself. I am sure that there will be plenty things you can thing out around this.\nP.S.: Using unsupported DLL included in BC installation After publishing this article, I got info from Steffen Balslev from Microsoft (Thanks!), that there is DLL for handling the .bclicense file I can use to read the info. Of course, this dll and using it in this way is unsupported and undocumented, it means you are using it on your own responsibility. In reality, there are two DLLs - one for .netstandard (Microsoft.Dynamics.BusinessCentral.Bcl.dll) and one for .net framework (Microsoft.Dynamics.BusinessCentral.BclFwk.dll). I succeeded to use the .net framework version from PowerShell to handle the license file. The .netstandard one is missing some reference (but it could be my fault).\nHere is my PowerShell code which loads the dll for you:\n#Download latest w1 artifact $ArtifactPath = Download-Artifacts -artifactUrl (Get-BCArtifactUrl -type OnPrem -country w1 -select Latest) -includePlatform #Find the dll $DLLPath = Get-ChildItem -Path $ArtifactPath[1] -Filter \u0026#39;Microsoft.Dynamics.BusinessCentral.BclFwk.dll\u0026#39; -Recurse -File #region Load the assembly $MSAzureKeyVaultPath = Get-ChildItem -Path $ArtifactPath[1] -Filter \u0026#39;Microsoft.Azure.KeyVault.dll\u0026#39; -Recurse -File |Select-Object -First 1 $NewtonsoftJsonPath = Get-ChildItem -Path $ArtifactPath[1] -Filter \u0026#39;Newtonsoft.Json.dll\u0026#39; -Recurse -File |Select-Object -First 1 $MSAzureKeyVault = [Reflection.Assembly]::LoadFile($MSAzureKeyVaultPath.FullName) $NewtonsoftJson = [Reflection.Assembly]::LoadFile($NewtonsoftJsonPath.FullName) #Some magic to resolve dependencies $OnAssemblyResolve = [System.ResolveEventHandler] { param($sender, $e) foreach($a in [System.AppDomain]::CurrentDomain.GetAssemblies()) { if ($a.FullName -eq $e.Name) { return $a } } if ($e.Name -like \u0026#39;Microsoft.Azure.KeyVault*\u0026#39;) {return $MSAzureKeyVault} if ($e.Name -like \u0026#39;Newtonsoft.Json*\u0026#39;) {return $NewtonsoftJson} return $null } [System.AppDomain]::CurrentDomain.add_AssemblyResolve($OnAssemblyResolve) try { Write-Host \u0026#34;Loading assembly\u0026#34; Add-Type -Path $DLLPath.FullName -Verbose }catch { Write-Host \u0026#34;Exception:\u0026#34; -ForegroundColor Red $_.Exception.LoaderExceptions } [System.AppDomain]::CurrentDomain.remove_AssemblyResolve($OnAssemblyResolve) #endregion The final use of the loaded assembly could be like this:\n$LicenseFile = \u0026#39;my.bclicense\u0026#39; $stream = [System.IO.StreamReader]::new($LicenseFile) $reader = [Microsoft.Dynamics.BusinessCentral.License.BcLicense.LicenseReader]::new($stream.BaseStream) $reader.GetObjectRangePermission(\u0026#34;Page\u0026#34;,18) Output will be like this:\nRangeStart : 4 RangeEnd : 56 Read : Direct Insert : None Modify : Direct Delete : Direct Execute : Direct Expiry : In this way, you do not need to parse the values etc. Everything is done by the class created for this purpose. But you are not in charge of the code. Choose wisely!\n",
    "ref": "/posts/bclicensestructure/"
  },{
    "title": "How to translate your AL app from Devglish to Endglish",
    "date": "",
    "description": "",
    "body": "Since we started to use VSCode for AL development, I hear discussions that writing UI texts like ToolTips etc. should be done by someone else than developer. And I must agree. We - developers - are lazy kind and writing text is not our popular game. We are able to write some generic text, mostly automagically generated by different tools we are using to write code, but such a text is mostly equal to 0 Shannons (no, I am not talking about @Shannon Mullins). But how to solve this?\nText written by developer Ask Microsoft to solve it On many occasions was this subject mentioned and discussed with Microsoft product group. I am sure that they are aware of this, but the list of To-Do is too long and nobody knows where in the list is this to solve. And I understand that, because there are things which are more important. But may be, once we will get something which will solve that.\nDo you know the \u0026ldquo;Blend for Visual Studio\u0026rdquo;? It is tool for designers to create the design of the app, without solving the code behind. Visual Studio is used for writing the code, and let the design be done in the Blend. Of course, we do not need such a huge tool.\nAnother example how these things are solved in other applications are the \u0026ldquo;placeholders\u0026rdquo; (X++ in Dynamics 365 AX was using this if I remember correctly and I assume that the successors are still using it). Developer just use some kind of code instead text and the text is assigned to the code somewhere outside the code. But it is much harder to understand the code, when you see only something like Message(\u0026quot;@SYS654433\u0026quot;).\nUse what we have But today I had discussion with my colleague Martin, and we were thinking how to put our consultants in charge of the content of our ToolTips etc. Of course, one way was to give them access to the code and let them overwrite the texts in the code. And I do not like it (of course, they will need e.g. to have license for Azure DevOps to be able to work with the repos etc.). But then I realized, that there is already one way, which our consultants are using to manage the texts - translations. They have possibility to do the translations to different languages as needed, because the XLIFF files for our apps are available and could be updated and then developers can incorporate the translations into new versions of the apps. It means, we already have everything what we need.\nHow? Just create new translation file for en-us language (there are multiple vscode extensions which could help you with the xliff files management - e.g. XLIFF Sync or NAB AL Tool) and do the ENU to ENU translation. This time it will not be about translating the text as it is to different language, but rather create new, better text describing all what end-user needs to see. In this way, developers could put \u0026ldquo;placeholders\u0026rdquo; into the code, e.g. just simple description what is going on, which text it is etc. (like \u0026ldquo;Tooltip for field My New Shiny Value\u0026rdquo;) or just use the generic values as you are used to. No problem. And then, let the consultants translate these \u0026ldquo;technical\u0026rdquo; texts into the real helpful end-user-centric texts. I am sure, that you already have processes which covers translation of your apps. And if something will not be \u0026ldquo;translated\u0026rdquo;, users will see the standard text from developers. And consultant could search for this text in the en-us translation file and change it as needed, you do not need to fix that in code (if it is not issue of fact etc.). Just update the translations, publish new app version and you are done!\nConclusion I see as best practice to have en-us XLIFF file in each app to solve this translation from \u0026ldquo;Devglish\u0026rdquo; (Developer\u0026rsquo;s English) to \u0026ldquo;Endglish\u0026rdquo; (End-user English). Let developers write the text in code in Devglish, and let consultants add the Shannons to the texts to transfer information to the Users and create the new Endglish layer.\nTranslation from Devglish to Endglish Downside There is one downside of this: your translation sources for other languages will be based on the Devglish version, not the Endglish version of the EN texts. But it is solvable and we will see, if some vscode extension will add functionality allowing to update the translation files from .en-US. version instead .g. version of the xliff. It means having the update chain changed from\nflowchart LR\r.g. --\u003e .cs-CZ.\r.g. --\u003e .sk-SK.\rto this flowchart LR\r.g. --\u003e .en-US.\r.en-US. --\u003e .cs-CZ.\r.en-US. --\u003e .sk-SK.\rIt will allow us even to update the translation in other languages when someone change the Endglish version and thus keep in sync the languages. I believe in BC community in this!\nP.S.: Do not put the Endglish translations into the .g. file directly, they will be lost when you compile the app, because .g. file is generated file, it is not source file. This is why it should not be part of the source code repository (add it into .gitignore file).\nFeedback If you have anything to add, comment or you want to share your way of solving these things, do not hesitate and put your comments here! I am looking forward the feedback and I am open to discuss this!\nThanks!\n",
    "ref": "/posts/whytowriteenutranslation/"
  },{
    "title": "Unattended access to local resources - WCF Relay",
    "date": "",
    "description": "",
    "body": "In many cases you need to work with resources (files, scales, card readers etc.), which are available on local network/PC, from within Microsoft Dynamics 365 Business Central which runs somewhere in the Cloud. I will show you one way how to solve these things.\nWhat is the problem? Different processes in Business Central can need some local resources of yours. In a simples case it could be some file on your disk on your PC (or server on local network). Standard way is, that BC client will ask user to select the file which should be uploaded or select where to save downloaded file. It needs user interaction. But many automatic processes needs to run without this user interaction. E.g. to monitor some local folder for new files, read them, process them and move them to archive. You can use some Azure Storage to be able to do that in compatible way, but in some cases it is not possible, for example when the file is generated by some ancient technology or some local hardware. Still you have possibility to \u0026ldquo;synchronize\u0026rdquo; these local files to the cloud storage by some technology, but it have own problems (we were using OneDrive synchronization for this, but the reliability was very low, mainly because we needed pseudo-online updates).\nHybrid solution To solve this problem, we can use some local running component, which have access to the local resource, and somehow connect it with the Cloud. \u0026ldquo;Standard\u0026rdquo; solution will be to have service, which offer some API. But because you need to connect from Cloud (from internet), you will need to somehow tunnel the outside incoming traffic to the local service. It will mean to create \u0026ldquo;hole\u0026rdquo; in your firewall to allow this. To limit possible security issues you can try to limit this hole for specific IPs from which the traffic will be allowed, but it is hard to do that for Business Central, because I do not know any specific IPs which are used for the BC services. Hybrid connection diagram Luckily we have different Azure Services, and one of them is solving this issue - Service Bus (specifically WCF Relay). This service is allowing your service to work in \u0026ldquo;opposite\u0026rdquo; direction - your service is connecting to the cloud from inside your network. It means you only need to allow outgoing connection in your local network which is much easier (and mostly is not limited, because it is standard connection to https endpoint on internet). Azure Relay will create the public endpoint on which you can consume the API you defined in your locally running service. To implement this you need only few settings in the app.\nOf course, it have some cost. You are paying some small amounts per each \u0026ldquo;relay hour\u0026rdquo; - it means for each hour your service is running and is connected to the Azure Relay (I can see standard cost 0.0090 EUR per 100 relay hours and 0.009 EUR per 10 000 messages).\nImplementation To implement this solution, you need to follow these steps:\nImplement Windows Service with Rest API with local endpoint Implement WCF Relay to have public endpoint Implement BC connector for this service Windows Service with Rest API How it could look like you can see here: LocalFS Service\nI am not expert in c#, thus take it as a work of beginner based on examples found on internet (like this).\nThis windows service will open one local endpoint on port selected in the config. Second endpoint will be created through Azure WCF Relay.\nService have 3 main areas:\nREST API (contract and implementation of the API) Windows service handling (main program and actions to register/start/stop the service) Using the WCF Relay to create public endpoint WCF Relay To be able to connect to the locally running service from outside your local network, you need to connect it to Azure Service Bus. To be able to do this, you need to create the Relay namespace in Azure portal. Creating Relay namespace The namespace is creating the URL which is unique for your relay. The public endpoint you will need to connect to, will have name like this:\nhttps://{RelayNameSpace}.servicebus.windows.net/{RelayServicePrefix}/{something}\nWCF Relays could be of two types: Dynamics or Static. Static can be created on the portal, but we will rather use the dynamic ones. They are created and removed automatically when you start/stop your service. RelayServicePrefix in the URL is any string you will choose. In this way you can group multiple different services together. The last part of the URL is again text you choose to make the URL unique. In my example code I am using computer name and domain name to create this part (like \u0026ldquo;PC0123.mydomain.local or \u0026ldquo;PC0123.\u0026rdquo; if there is no domain).\nTo be able to register the service into the relay, you need to use Shared access policy key. You can get it on the Azure portal in the Shared access policies section. Getting SAS key When you set the values and run the service, you should see the service in the WCF Relay list on the Azure portal.\nBusiness Central connector To connect to the service from Business Central, use standard HTTPClient to connect to the relay address. It is standard REST API, thus it should not be a problem for you to do that. If you want to use this LocalFS service, you can use our AppSource app Navertica Local FS Connector.\nI am sure that you will find many examples how to implement REST API client in AL on internet.\nSecurity The WCF Relay is transparent and it means it is on your service to make the authentication of the caller. In the example I am using just basic mechanism which checks that the used user name and password is same as configured one.\nI am sure that there is some possibility to add security on the WCF Relay, but have not time to dig into it yet. If you have some tips, you can use the comments to share them.\nConclusion I wanted to show you possible way how to handle things which were blockers some time ago for using Business Central Online. You can easily modify the service to not handle local files access but e.g. communicate with some locally connected hardware (special printers, scanners, scales etc.). Such a things should not prevent you now from going to cloud. It have some cost (cost of the WCF Relay), but I think it is really small cost which is overweighed by what you will get.\nI know that there are other ways how you can solve such a cases, but this one is for me easy to use even with my limited knowledge. I hope it will help you to think outside the box and unblock your way.\nSee you in the Cloud!\n",
    "ref": "/posts/localfs/"
  },{
    "title": "Snapshot debugger on OnPrem - what you need to know to set it up",
    "date": "",
    "description": "",
    "body": "When you want to use Snapshot Debugging on your OnPrem environment, may be you can hit this error when trying to initialize it:\nError: The SSL connection could not be established, see inner exception.\r... In some cirumstances you can even have problem to start the BC Service when you enable the Snapshot Debugger endpoint.\nWhat is the problem? For standard endpoint, which we are using for longer time already, like client endpoint (default port 7046), OData (7048) and Dev (7049), the Management console for BC Server is automatically setting some things in background when you save the settings in it. But for the Snapshot debugger, this is not done and you need to do it yourselfs.\nURLACL First thing is, that for the user account, under which is BC Server running, usage of the HTTP address is reserved (see this). Manually it could be done by running this command as admin on the server:\nnetsh http add urlacl url=https://+:7083/BC/ user=\u0026#39;NT AUTHORITY\\NETWORK SERVICE\u0026#39; You need to modify the URL if you are not using SSL (use http instead https) and port number/instance name. User needs to be the one under which the service is running.\nTo list existing urlacl you can use this command:\nnetsh http show urlacl If this is not done, you will ger some error about \u0026ldquo;unable to listen\u0026rdquo; on the specified address in your event log.\nSSLCERT Second step is to assign certificate to the address, if you are using SSL for the endpoint.\nTo use SSL it is not enough to set the certificate in the BC Service Management console (in the BC Service configuration), but the certificate must be assigned to the address.\nThis is done again through netsh command:\nnetsh http add sslcert certhash=\u0026lt;thumbprint\u0026gt; appid=\u0026#39;{00112233-4455-6677-8899-AABBCCDDEEFF}\u0026#39; ipport=0.0.0.0:7083 Use the thumbrint of the certificate you want to use (e.g. same like in the BC Service configuration), appid could be any GUID (e.g. like in th example), and ipport must be the port for which you are assigning the certificate.\nTo list existing assignments you can use this command:\nnetsh http show sslcert If you want to assign certificate to port, where already is certificate assigned (e.g. when renewing expiring certificate), you need to delete the sslcert first:\nnetsh http del sslcert ipport=0.0.0.0:7083 Some hicups Sometimes, when you are changing the SSL settings on the BC server, you can get into issues that when saving the configuration you get some errors that Certificate cannot be registered or something similar. It is mostly in situations, when the settings and the current state of the SSLCERT is not in line - it means, you are disabling SSL but SSLCERT is not registered (e.g. after you have copied configuration from another instance having SSL enabled and you current instance had it disabled) or vice versa. In this case you need to fix this discrepancy (when disabling SSL, create the SSLCERT assignment first, when enabling, remove the existing assignemnt first).\n",
    "ref": "/posts/snapshotdebuggeronprem/"
  },{
    "title": "Architecture of PTE - Split or not to Split?",
    "date": "",
    "description": "",
    "body": "On Directions EMEA in Milan I delivered session about Architecture of PTE for Business Central. I was trying to find some rules when or why you should or shouldn\u0026rsquo;t split your PTE into multiple apps. Now I will try to catch this into this article.\nSplit or not to split Some partners are putting everything into one app per customer. Some are splitting all to separate apps based on different rules like per process, per area etc. Our company is going through the way of splitting, rather than not splitting, because on the beginning we wanted to have possibility to re-use the \u0026ldquo;tiny apps\u0026rdquo; for others if needed. But the result? Here is the picture (taken from real live customer project):\nReal world apps dependencies example As you can see, some apps are \u0026ldquo;self standing\u0026rdquo;. This is what we wanted to have. But most of them are connected with others. That\u0026rsquo;s something we didn\u0026rsquo;t want. What does it mean? They are not re-usable, if you do not want to re-use even the dependencies. It means you can reuse only the \u0026ldquo;leftmost\u0026rdquo; apps. It makes most of the apps non-reusable without refactoring.\nBut the question stays: Split or not to split?\nTypes of PTE apps based on the chart, I made some analysis and found these sets of apps based on names of the apps:\nTOOLS/LIBRARIES - \u0026ldquo;Record links\u0026rdquo;, \u0026ldquo;Control Focus\u0026rdquo;, \u0026ldquo;Barcodes\u0026rdquo;, \u0026ldquo;PDF Signature\u0026rdquo; IMPORTS/EXPORTS - \u0026ldquo;Data Migration\u0026rdquo;, \u0026ldquo;Salary Import Connector\u0026rdquo;, \u0026ldquo;Banking\u0026rdquo; CONNECTORS - \u0026ldquo;Manufacturing API\u0026rdquo;, \u0026ldquo;Laboratory Integration\u0026rdquo;, \u0026ldquo;NAV Web Service Adapter\u0026rdquo; SIMPLE PROCESSES - \u0026ldquo;VAT Posting Date Change\u0026rdquo;, \u0026ldquo;Standard Cost Management\u0026rdquo;, \u0026ldquo;Approval\u0026rdquo;, \u0026ldquo;On Hold\u0026rdquo; COMPLEX PROCESSES OR AREAS - \u0026ldquo;Service Acceptance\u0026rdquo;, \u0026ldquo;Spare Parts\u0026rdquo;, \u0026ldquo;Overhead Material\u0026rdquo;, \u0026ldquo;FA Inventory\u0026rdquo;, \u0026ldquo;Price Security\u0026rdquo;, \u0026ldquo;Manufacturing\u0026rdquo;, \u0026ldquo;Report Pack\u0026rdquo;… EXTENSIONS (CUSTOMIZATIONS) - \u0026ldquo;Handling Unit Extension\u0026rdquo;, \u0026ldquo;Continia Extension\u0026rdquo;, \u0026ldquo;Small modifications\u0026rdquo;, „Intrastat Extension\u0026quot; We can define some properties of each type, which could help us to identify them:\nTOOLS/LIBRARIES - Adding functionality which is used through whole system, independent on specific processes/areas IMPORTS/EXPORTS - working with files or other data sources CONNECTORS - connecting with other systems (through API - as client or as server) or between extensions SIMPLE PROCESSES - mostly adding some small tasks and processes, limited schema changes (not many fields and tables) COMPLEX PROCESSES or AREAS - creating new processes or whole areas, new tables, pages… EXTENSIONS (CUSTOMIZATIONS) - are modifying app which is in responsibility of someone else (3rd part, different product group etc.) And based on their properties, we can divide them into two groups:\nNot Problematic TOOLS - mostly depending on standard only (lowest level dependencies) IMPORTS/EXPORTS - low probability of depending apps, rather depends on other (similar to CONNECTORS), sometimes need to be OnPrem if we are OnPrem CONNECTORS - should be top (or bottom) app in the tree Problematic SIMPLE PROCESSES - you do not know if it will not grow to be complex COMPLEX PROCESSES or AREAS - hard to split the processes correctly and solve the dependencies when cyclic EXTENSIONS (CUSTOMIZATIONS) - could have depending apps, could grow to complex processes But still: Split or not to split?\nDifferences Ok, previous classification is not helping us to answer the question. We should look at the CONs and PROs of the splitting the PTE into separate Apps:\nSplitting PRO: Easier parallel development (development split to more apps) Clear responsibilities Simpler/Smaller apps Simpler maintenance Re-use (really?) List of functionalities Possibility to „uninstall\u0026quot; customization when not needed (really?) CONS: Dependency „hell\u0026quot; (solvable) Architecture decisions are more complex Performance (solvable) Release complexity (solvable) Cost of maintenance per app (partially solvable) We can discuss if the apps are re-usable (we already touched this). But most of the CONs are solvable by using correct tools (partial records etc.) and automatic processes (release pipelines, powershell etc.).\nAll-in-one PRO: Simplified Architecture Simplified release to Customer Cost of maintenance per app Performance (! not because technical issues, but because mindset of the developers - \u0026ldquo;it is in one app, why bother with performance?\u0026rdquo;) CONS: Developers conflicts (solvable) Release Planning (solvable) Installation times (? in history, deploying big app took more time) Impossible to re-use without refactoring What is inside (solvable) Shared responsibilities And again, you can see that most of the CONs could be solved by tools (AL Object ID Ninja, documentation etc.) and processes (Azure DevOps planning etc.)\nStill: Split or not to split?\nComparing Ok, compare the PROs and CONs of both methods when we remove the solvable items:\nPROs and CONs comparison Items I think are most interesting are in bold.\nAll depends on priorities you have in your company or team. If your priority is e.g. \u0026ldquo;Simpler maintenance\u0026rdquo; and \u0026ldquo;Clear responsibility\u0026rdquo; and you sacrifice the \u0026ldquo;easy architecture decisions\u0026rdquo;, go and split the apps.\nIf your priority is simpler \u0026ldquo;architecture decisions\u0026rdquo; and you sacrifice \u0026ldquo;shared responsibility\u0026rdquo; (who is responsible for the content of big, all-in-one app?), go and do not split.\nOk, does it mean split or not to split?\nSMART Architecture For me, both ways are valid. But you need to do it SMART.\nSIMPLE MANAGABLE ARCHITECTURE REACHING TARGET And to do it, you need to know what the target is for you, your team, your company, your customers.\nWhat it could mean? For example this:\nKeep TOOL/LIBRARIES and CONNECTOR apps separate Use Dependency Inversion principles (interfaces etc.) where you want to \u0026ldquo;reverse\u0026rdquo; the dependency Try to keep the responsibilities clear. Boundary of App is boundary of responsibility. Public interface of App is contract between apps. When you need to depend on new App (3rd party etc.), try to create CONNECTOR as separate app to prevent adding the dependency into the \u0026ldquo;core\u0026rdquo; app if possible. When you need to switch the app to „OnPrem\u0026quot;, create CONNECTOR (working with local files etc.) - leave rest of the logic in „Cloud\u0026quot; (and best is to not use OnPrem at all - it will cost you, literally) Content of the app must be in line with app name. If the change is not in line with the app name, may be it should be somewhere else. Choose wise the name of the app. Conclusion May be you have still questions, what is THE BEST way. But I think there is no silver bullet for this. The answer could be different, depending on who you ask. But if you already go some way, do not switch just because someone tells you that it is better in another way. May be he/she is solving different problems than you and your solution in your situation is the best for you.\nI hope that this article helps someone to think about the architecture of Business Central PTEs from different angles, and I am open to discussion about the architecture at all. What I see now as biggest gap in our community, is lack of architects and discussion about the architecture itself. And this my session and article is for me starter for the discussion we will have next years.\nBe safe! See you online/onprem!\n",
    "ref": "/posts/ptearchitecture1/"
  },{
    "title": "My new blog",
    "date": "",
    "description": "",
    "body": "New blog May be you already noticed that my original blog on dynamicsusers.net (original URL was https://www.dynamicsuser.net/nav/b/kine) is gone. I am building new one from scratch. May be it is time to begin something new and cut off the Dynamics NAV history.\nBecause todays it is easy to use GitHub as a platform for the blog (and thus using git and vscode to \u0026ldquo;post\u0026rdquo; the articles), I decided to use HUGO as the framework. Another side of this is, that my blog is \u0026ldquo;opensourced\u0026rdquo;, thus you can see everything what is there and if you have some need to fix something, you can easilly post pullrequest.\nI hope that you will find it worth to read it.\nEnjoy it!\n",
    "ref": "/posts/newblog/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "About me I have more than 35 years of experience with software development and IT in general. In the year 2001, after finishing university, I joined the NAV world in NAVERTICA company as NAV developer. Having a wide area of knowledge in programming languages and connected areas is helping me to understand “how it works” and is giving me background for solving different tasks. For the last years, I am working more with GIT, Powershell, and Azure DevOps. I am a Microsoft Most Valuable Professional (MVP) since the year 2005.\nWhat I know I have learned mostly on my own (many trials\u0026amp;errors), because I started when there was no internet and even no books about computers. Our country lived in communism and my first computer Atari 800XE was bought in Austria (thanks to my parents for this decision). But all that gave me something, what cannot be taught.\nIn personal life I have beloved wife and my free time activity is to play PC games/simulators like IL-2 Sturmovik, MS Flight Simulator and other.\n",
    "ref": "/about/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "If you need professional services from me, contact NAVERTICA a.s. sales. I am employee in the company and I am not able to serve you directly.\nIf you want to support me personally, you can use the BuyMeCoffee tipping system.\n",
    "ref": "/contact/"
  }]
