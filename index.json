[{
    "title": "Accessing Azure Blob Storage with Service Principal from Business Central AL",
    "date": "",
    "description": "",
    "body": "Azure Blob Storage from AL After long time I needed to solve some AL problem for my colleagues and I was not able to find the solution on internet and even from other MVPs. Today I had some time to dig into the problem and I have solved it. I want to share the solution with you. But first, we need some introduction into the problem.\nIt is common to use Azure Blob Storage to store data and work with them from Dynamics 365 Business Central. Developers can use system app codeunit \u0026ldquo;ABS Blob Client\u0026rdquo; to call different methods of the storage. But this library implements only Shared Key (mostly used because it is the simples one) and SAS authorization. But the most recommended method is to use Microsoft Entra ID with managed identities to authorize the calls. And because our customers are taking the security seriously, we wanted to change the code from Shared Key auth to Service Principal (managed identity). I started to search for some example for AL regarding this, but very example is using just Shared Key auth.\nOk, time to do own research and learn something new. During the search I found some example in Python and based on this example it looked like easy task. When I connected the example with my knowledge about OAuth, the solution started to crystalize.\nManaged Identity auth for Storage Service During analysis of the \u0026ldquo;Azure Storage Service\u0026rdquo; implementation in System application I hit the interface \u0026ldquo;Storage Service Authorization\u0026rdquo; which is used to do the authorization of the API calls. This interface defines only one procedure which needs to be implemented:\nprocedure Authorize(var HttpRequest: HttpRequestMessage; StorageAccount: Text); When I looked on existing implementations, this method needs to set the HTTP Attributes of the request as needed to authorize the request. In our case, we need to set the Authorization header to \u0026ldquo;Bearer token\u0026rdquo;, where the Token is OAuth token for our Managed Identity for accessing the resource we need to access. Based on the examples I found it is just standard authorize OAuth call with client credentials where:\nClientID is the ID of the managed identity (Entra ID app we registered for this purpose) ClientSecret is the secret for he managed identity AuthorizationURL is e.g. https://login.microsoftonline.com/tenantid/oauth2/authorize/ RedirectURL could be empty (we are not using delegation) ResourceURL is https://\u0026lt;accountname\u0026gt;.blob.core.windows.net/ Thanks to codeunit OAuth2 it is easy to do this call by using this code:\nif not OAuth2.AcquireTokenWithClientCredentials(ClientId, ClientSecret, AuthURL, RedirectURL, ResourceUrl, Token) then Error(TokenNotAcquiredErr); exit(Token); Then we can create own implementation of the \u0026ldquo;Storage Service Athorization\u0026rdquo; interface:\ncodeunit 50100 ServicePrincipalAuth implements \u0026#34;Storage Service Authorization\u0026#34; { var Token: SecretText; ClientId: Text; ClientSecret: SecretText; AuthURL: Text; RedirectUrl: Text; ResourceUrl: Text; TokenNotAcquiredErr: label \u0026#39;Failed to acquire token\u0026#39;; ​ procedure Authorize(var HttpRequest: HttpRequestMessage; StorageAccount: Text) var Headers: HttpHeaders; AuthToken: SecretText; begin if Token.IsEmpty() then Token := GetToken(ClientId, ClientSecret, AuthURL, RedirectUrl, ResourceUrl); HttpRequest.GetHeaders(Headers); if Headers.Contains(\u0026#39;Authorization\u0026#39;) then Headers.Remove(\u0026#39;Authorization\u0026#39;); AuthToken := SecretStrSubstNo(\u0026#39;Bearer %1\u0026#39;, Token); Headers.Add(\u0026#39;Authorization\u0026#39;, AuthToken); end; ​ procedure SetPrincipalData(_ClientId: Text; _ClientSecret: SecretText; _AuthURL: Text; _RedirectURL: Text; _ResourceUrl: Text); begin ClientId := _ClientId; ClientSecret := _ClientSecret; AuthURL := _AuthURL; RedirectUrl := _RedirectURL; ResourceUrl := _ResourceUrl; end; ​ local procedure GetToken(ClientId: Text; ClientSecret: SecretText; AuthURL: Text; RedirectUrl: Text; ResourceUrl: Text): SecretText var OAuth2: Codeunit OAuth2; begin if not OAuth2.AcquireTokenWithClientCredentials(ClientId, ClientSecret, AuthURL, RedirectURL, ResourceUrl, Token) then Error(TokenNotAcquiredErr); exit(Token); end; } When we have this implementation, the code for e.g. listing the Blobs is just few lines of code:\nServicePrincipalAuth.SetPrincipalData(ClientId, ClientSecret, AuthURL, RedirectURL, ResourceUrl); ABSBlobClient.Initialize(AccountName, ContainerName, ServicePrincipalAuth); ABSOperationResponse := ABSBlobClient.ListBlobs(ABSContainerContentTemp); This implementation is not handling expiration of the token, it is bare minimum to do the call at least once.\n",
    "ref": "/posts/azureblobstoragewithserviceprincipalinal/"
  },{
    "title": "Using Paket CLI for Business Central",
    "date": "",
    "description": "",
    "body": "In my previous article NuGet-izing Business Central: Current Status I have mentioned NVRAppDevOps powershell module we are using in our DevOps and that it supports using NuGet packages together with Microsoft Dynamics 365 Business Central. This article will be about how it works.\nPaket CLI When we were starting to use NuGet packages for our BC apps, I was using nuget.exe tool only. Later, when we had some additional requirements, I have found Paket CLI and learned that it is much easier to use for my purpose. Thus we migrated to this tool for the consuming part of our pipeline (the process, when we need to download the dependencies to be able to compile our app or deploy it to new contianer etc.). It is much easier to use than nuget.exe, because it is not so tightly coupled with Visual Studio project like nuget.exe is.\nAnd by using existing tool, developed by and for community which is much bigger and \u0026ldquo;older\u0026rdquo; (in using NuGet) than BC community, I can use the know-how already encoded into this tool for free and right now. Downloading the NuGet packages is not so simple as it can look on the first sight. Why to reinvent the wheel?\nHow Paket works Paket.dependencies file To use Paket, you need only the paket.exe and one text file with name \u0026ldquo;paket.dependencies\u0026rdquo;. This file is describing all what you need to be able to download the dependencies. Description of the file is documented here.\nExample could look like this:\nstrategy: min # use the minimum version of transitive dependencies lowest_matching: true # use the lowest matching version of a direct dependency source https://myaccount.pkgs.visualstudio.com/_packaging/myfeedname/nuget/v3/index.json username:\u0026#34;\u0026#34; password:\u0026#34;%PAT%\u0026#34; authmethod:basic nuget NAVERTICAas.NaverticaMeters.71971f57-xxxx-xxxx-xxxx-5e0886784631 \u0026gt;= 21.0.0.0 nuget Microsoft.Application \u0026gt;= 23.3.0.0 strategy:min, lowest_matching:true nuget Microsoft.Platform \u0026gt;= 23.0.0.0 strategy:min, lowest_matching:true Source in our case is some Azure DevOps artifact feed. The URL could be found on the \u0026ldquo;Connect to Feed\u0026rdquo; info on your feed in Azure DevOps. After you click on this button, just select that you want to connect to NuGet e.g. frm Visual Studio and on the opened page you will see the Source where the URL is ready to copy. When using private feed, you need to set even the authentication mode to Basic and set password to some PAT. You can use system environment variable to get the value to not include it directly in your file, because the paket.dependencies could be commited as part of your source code. Just set the PAT environment variable to the PAT value and you will be able to use this source to download your packags.\nStrategy and lowest_matching are options which sets version policy when resolving the dependency tree (if you want min or max versions for transitive dependencies and direct dependencies). Do not forget that you should use both policies in your pipelines, because both are valid and are testing or giving you different aspect of your application (see this Areopa webinar for more info).\nLast lines are telling Paket which dependencies we want to download. Paket will automatically resolve whole dependency tree and versions of all the dependencies based on the requirements you set here. Paket is able to work even with other packages than NuGet, but we will be using only the NuGet packages for now.\nYou can see that it is not hard to generate this file based on the app.json of your app and we need only somehow add the sources. How we will generate the package names are given by the Rules we have defined. The rule is simple:\n- pattern: \u0026lt;Publisher\u0026gt;.\u0026lt;AppName\u0026gt;.\u0026lt;Tag\u0026gt;.\u0026lt;AppId\u0026gt; - max length: 100 chars. If longer, cut \u0026lt;AppName\u0026gt; to fit - allowed characters (except separator \u0026#39;.\u0026#39;): a-zA-Z0-9_\\- Tag could be localization tag when the app exists with same AppId in multiple \u0026ldquo;flawors\u0026rdquo; (mostly only MS Apps will use this) or it could be \u0026lsquo;symbols\u0026rsquo; if the package container only symbols or \u0026lsquo;runtime\u0026rsquo; if it is runtime package. Else it is not used at all.\nPaket.lock file During usage of Paket, the Paket.lock file is created. This file describe which versions of dependencies were used last time you resolved them. If you include this file as part of your repository and you will use only \u0026ldquo;Paket restore\u0026rdquo; command (see next section), you will get same versions of the dependencies every time. This can help you to make the process consistent and free from side effect of using different versions of dependencies every time you resolve them.\nPaket commands When running Paket, you need to choose which command you want to run:\nPaket \u0026lt;command\u0026gt; is one from this list:\nInstall - Compute dependency graph, download dependencies and update the folder. Will resolve new packages in the paket.dependencies and use already used versions for already resolved dependencies. Only new packages are added into paket.lock file, if exists. Update - Update dependencies to their latest version - use this when you want update the dependencies. Process will resolve the versions again and will store the new versions into paket.lock file. You can update only selected package if you want. Restore - will donwload the computed dependency graph as described by the paket.lock file. It means it will use same versions as when the paket.lock file was updated last time. No new versions will be resolved or updated. clear-cache - could be used to clear the local cache, e.g. when you unlist some packages on the source etc. and there are other commands, but we do not need them right now.\nWorkflow with Paket create paket.dependencies based on the app.json and add the source for the feed you want to use run \u0026ldquo;Paket install\u0026rdquo; to resolve the dependency graph when there are new dependencies in the file run \u0026ldquo;Paket update\u0026rdquo; when you want to update version of some/all dependencies and download them run \u0026ldquo;Paket restore\u0026rdquo; when you want to download same versions of dependencies as described by paket.lock file (restore the last state) NVRAppDevOps and Paket Usage from VSCode In Powershell module NVRAppDevOps, there is cmdlet \u0026ldquo;Invoke-PaketForAL\u0026rdquo; which pack all the functionality into one command. You can use it like this:\nInvoke-PaketForAL -Sources \u0026#39;https://myaccount.pkgs.visualstudio.com/_packaging/myfeedname/nuget/v3/index.json username:\u0026#34;\u0026#34; password:\u0026#34;%PAT%\u0026#34; authmethod:basic\u0026#39; This is good e.g. for usage from VSCode, when you need to download AL symbols. Instead downloading symbols from some existing environment, you can download them as NuGet packages from your source feed.\nIf you run the command in same folder as app.json of your app is and the module will be able to run Paket.exe from default path, it should convert the info from app.json and create the paket.dependencies file for you, add the one source and will call Install paket command.\nThe internal flow of this cmdlet is this:\nIf paket.dependencies exists, read the content If no -Sources are passed as parameter, use the Sources already existing in the paket.dependencies file Update the paket.dependencies file with actual dependencies from app.json When creating NuGet package ID for Business Central application, first look through sources for package with the AppId in name. If Found, use this package name If no package with the AppId found on the sources, generate the NuGet package id based on the rules Add the dependency into the paket.dependencies file with the version limits from app.json Run the \u0026ldquo;Paket install\u0026rdquo; command. if system environment with name PaketExePath is set or the path is passed through PaketExePath parameter, use the value as path for the Paket.exe. If UsePackagesAsCache is specified, the folder Packages is added into al.packageCachePath setting in .vscode/settings.json file. ALC then will use the .app files inside this folder automatically as it is using .alpackages by default. It is good to add the Packages folder into .gitignore file, because it should not be part of the repository. Only the paket.dependencies and paket.lock should be commited into the repository.\nLater you can use only simpler version without source parameter, if the paket.dependecies already exists:\nInvoke-PaketForAL This will reuse existing paket.dependencies file with sources and will just regenerate the dependecies package IDs and version limits. Then it will issue the \u0026ldquo;paket install\u0026rdquo; command.\nUsag from DevOps CI/CD pipeline Same cmdlet from the NVRAppDevOps module could be used to download the .app files as part of your CI/CD. In this case, you will mostly use the \u0026ldquo;restore\u0026rdquo; command to restore the dependencies to state described in the paket.lock file. Or you can do other commands in connection with different policy parameter to download minimum or maximum versions of the dependencies.\nYou will probabbly reuse the sources already defined in the paket.dependencies file. You just need to set the environment variable used e.g. for defining password (when using e.g. %PAT% value as password) to authenticate correctly.\nAfter NuGet packages are downloaded, you can use the package folder as source of symbols for the alc to compile your app, or as source of apps to deploy to target BC environment (probabbly skipping the MS apps because they will be already in the environment).\nSummary I have created the Invoke-PaketForAl cmdlet to make the process of using NuGet packages in connection with Microsoft Dynamics 365 Business Central as easy as possible. Still, there is enough parameter to customize the result. I hope that i twill be the starting point to use the NuGet packages with BC instead downlaoding the symbols from existing environment. But still, we need at least the Microsoft NuGet feed for MS apps to have simple experience. See the \u0026ldquo;NuGet-izing Business Central: Current status\u0026rdquo; article about current status of the whole story. It could take some time, but I am ready for the future\u0026hellip; ;-) Whole process is compatible with standard usage of the Paket CLI and is just automatizing some aspects of this. It means you can use Paket directly if you need some specific features which are not covered by the cmdlet. Creating of paket.dependencies fiel could be called as separate cmdlet ConvertTo-PaketDependencies if you need like this:\nConvertTo-PaketDependencies -ProjectPath $ProjectPath -NuGetSources $Sources -Policy $Policy -MaxApplicationVersion $MaxApplicationVersion -MaxPlatformVersion $MaxPlatformVersion In case of some problems or requests, do not hesitate to creaet issue on the NVRAppDevOps GitHub repo.\n",
    "ref": "/posts/paketforbc/"
  },{
    "title": "NuGet-izing Business Central: Current status",
    "date": "",
    "description": "",
    "body": "In my last sessions on different conferences and webcasts I was talking about my dream regarding using NuGet technology to distribute and consume Business Central .app files. These sessions were starter point for make the whole dream reality. But we are not there yet. We just started the trip. This blog article want to summarize current status of this trip.\nRequirements To make the dream reality, we need multiple things to happen:\nDefine rules for creating the NuGet packages in connection with Microsoft Dynamics 365 Business Central - status: DONE! (see e.g. my and Freddy\u0026rsquo;s BCTechDays session \u0026ldquo;When a dream comes true\u0026rdquo;) Microsoft must publish their apps as NuGet packages on some public feed - status: Finished (updated 14.10.2024) - MS Apps public feed, MS Apps Symbols feed (optional) Microsoft need to publish AppSource apps symbols as public feed - status: Finished (updated 14.10.2024) - public Artifact feed - updated when publisher publish new version of some app Partners need to publish their apps as NuGet packages on some feeds following the rules - status: Depends on partner (unknown) Partners need to be able to consume NuGet packages following the rules during CI/CD and development - status: Depends on used technology (see next chapter) Adoption of NuGetizing for BC in DevOps solutions One part of the NuGetizing the BC is possibility to consume and produce the NuGet packages during your CI/CD pipelines.\nHere is the list of existing managed solutions and tools for DevOps for Business Central and their status of NuGet support:\nAlpaca: On the roadmap (WIP) ALOps: On the roadmap (WIP) BCCH (BCContainerHelper): WIP AL-Go for GitHub: WIP (updated 30.9.2024) NVRAppDevOps (powershell module): Done! (since version 2.8.4-beta20, use the Invoke-PaketForAl) If you know any other DevOps solution for Business Central, let me know and I will add it here.\nAdoption of NuGetizing for BC in VSCode Another aspect of NuGetizing Business Central is possibility to consume NuGet packages from VSCode to replace the \u0026ldquo;AL: Download Symbols\u0026rdquo; action.\nHere is list of tools which could help with this (updated on 14.10.2024):\nNVRAppDevOps (powershell module): Done! (since version v2.8.4-beta20, use the Invoke-PaketForAl) AL NuGet Helper from Patrick Schiefer (VS Code extension): Released first version 10.10.2024, WIP, inside using Paket CLI Summary You can see, that tools are popping up, but without the feed infrastructure from Microsoft and other partners, it is hard to use them. In our company we are using NuGet packages whole time we created our CI/CD pipelines for Business Central. Thus we have even our internal fake packages for MS Applications and we are wrapping the 3rd party .app files as NuGet packages for our internal needs. But it is just complication and it is not part of the dream (it is rather nightmare). When the feed infrastructure from MS will be finished, it will be much easier to start using the tools to incorporate NuGet packages into your processes.\nRegarding consuming the NuGet packages - if you are early adopter kind of person, you can start now and solve these things yourselfs. Rest needs to wait till the MS feeds are finished at least.\nRegarding producing NuGetPackages - you can start now, because at least two tools are already there for you - BCCH and NVRAppDevOps cmdlets (New-BcNuGetPackage in BCCH and New-ALNuSpecForAppFile, New-ALNuSpec and New-ALNugetPackage in NVRAppDevOps cmdlets). And even when you do not want to use these tools, it is easy to generate the XML .nuspec file and create the package by using current tools like NuGet.exe or Paket.exe CLI. There is no hidden magic. Just follow the rules for naming the packages. You do not need to consume the packages yet, but you can build your feeds in this way and be prepared later, when we will be closer to the goal.\n",
    "ref": "/posts/nugetizingbusinesscentralstatus/"
  },{
    "title": "SaLi architecture for Microsoft Dynamics 365 Business Central, part 2",
    "date": "",
    "description": "",
    "body": "See previous part first - SaLi architecture for Microsoft Dynamics 365 Business Central, part 1\nIn previous part we were talking about inter-application Dependencies and their layering which helps with correct dependency structure. But still we have a problem how to decide, which new requested feature will go into which application to keep the architecture clean. To help with this decision making, we created few rules regarding this.\nSaLi Naming rule Is there a unique and concise name for the app that describes the functionality? If not, it’s a signal that you may be merging multiple \u0026ldquo;apps\u0026rdquo; scopes into one.\nArchitect thinking about different names for the app This is first rule, which helps with correct naming of the apps you are creating. Because name of app is defining scope of the app. Creating correct name means defining correct scope. If I have clear and simple scope, I can later decide correctly if requested functionality match the scope or not (see Scoping rule). This helps with fast and correct architecture decision process.\nIf I cannot create some simple and clear name for the app, may be the scope I am trying to name is too complex or wide. Of course, this could be solved by using some \u0026ldquo;generic\u0026rdquo; naming like \u0026ldquo;Per tenant app\u0026rdquo; or \u0026ldquo;Small modifications\u0026rdquo; or \u0026ldquo;All what you want changes\u0026rdquo;, but all depends on your sense and scale of splitting your solution into more apps (Split or not to split). This rule is not solving this \u0026ldquo;Split or not to split\u0026rdquo; problem, it just emphasize synchronicity between app Name and app Scope.\nSaLi Scoping rule If you are adding new thing into app (function, object\u0026hellip;), ask \u0026ldquo;Is it in line with the naming of the app (function, object\u0026hellip;)?\u0026rdquo; If not, put it somewhere else or rename the app.\nWhere I will put this change? This rule is similar to the Naming rule, but other way around - app already exists and you are deciding if the new feature match the scope of the app or not. Target is to not put functionality into app if it is not in line with the scope of the app - it means keeping the app \u0026ldquo;clean\u0026rdquo; - containing only functionality which are in line with the purpose and name of the app.\nIf I have app \u0026ldquo;External CRM integration\u0026rdquo;, there must be only functionality fullfilling the generic External CRM Integration functionality, not e.g. some specific external CRM system requirements (these will be e.g. in separate app \u0026ldquo;SomeCRM Communication Interface\u0026rdquo;). As a result, some original functional requirements could be implemented as multiple functionalities added into multiple applications, because they requires changes in multiple scopes. At the end, you will need to split the requirement into multiple change requests for multiple apps (multiple responisibilities). This could be seen as complication, but we think that it is just helping with decomposition of the problem to smaller pieces which are mostly loosly-coupled (see Decoupling rule).\nSaLi Sharing rule When you add field (action/process) to a process/in-out app, think, ”Will you need it elsewhere when you uninstall that app?” if so, don’t put that into this app, but into some app in SHARED/DISCRETE layer.\nSharing data between applications Placing data/processes/actions into correct layer (app) is helping with the whole architecture. E.g. if you place the field with some data into app in \u0026ldquo;Input/Output\u0026rdquo; layer, no othe app will have access to it, because othe apps cannot have dependency on \u0026ldquo;Input/Output\u0026rdquo; layer apps. Sometime it is correct (value specific for this specific app which is only for internal purpose) and this rule is trying to help us decide, if it is true in specific case or if this specific data/process will be \u0026ldquo;shared\u0026rdquo; with other apps. If yes, we need to push this change into apps in lower layers to be able to share it easilly. Do not forget, that lower layers are for \u0026ldquo;generic rules, business processes etc.\u0026rdquo;, higher layers are for \u0026ldquo;specific things\u0026rdquo;. This rule helps with implementing the generic things first and then implement their specific implementations.\nPart of this decision is even decoupling generic data/processes from specific data/processes (connected to specific external system, internal app, etc.). E.g. field \u0026ldquo;Exported (count)\u0026rdquo; on sales document (which drives some internal workflow processes) will be generic field in some lower layer app, but field \u0026ldquo;Exported to SomeCRM (count)\u0026rdquo; will be probably in \u0026ldquo;Input/Output\u0026rdquo; layer app for \u0026ldquo;SomeCRM\u0026rdquo; integration only and will not be connected to generic workflow processes (because these processes will not work when you switch from \u0026ldquo;SomeCRM\u0026rdquo; to \u0026ldquo;AnotherCRM\u0026rdquo; integration - it means when you uninstall \u0026ldquo;SomeCRM\u0026rdquo; integration app).\nThinking about \u0026ldquo;What will happen when I remove this app\u0026rdquo; helps you to prepare the solution to situations when you decide to change part of the system. If you think about this, solution will be prepared for future changes (this is part of the SOLID priciples).\nSaLi Decoupling rule If your functionality is expected to work without the app on which it depends, then don’t make that dependency (and do it with help of some library or connector app, split the app\u0026hellip;).\nWhat happen when I uninstall this? Rule is similar with the Sharing rule, but from another point of view. Thinking about \u0026ldquo;what happen when I remove ths app\u0026rdquo; helps with identification of tightly-coupled apps and validating if this relation is correct. Making all apps tightly-coupled means we have rigid system, which is hard to change (and it means against SOLID architecture). To make the system flexible means, we need to introduce more abstract apps (helping us to split the generic functionality from specific implementation) or implement different \u0026ldquo;bridge apps\u0026rdquo; inter-connecting multiple loosly-coupled apps. In case we need to remove/exchange one app, we just reimplement the bridge app, which is tightly-coupled with it. All this helps us to implement correctly the SOLID principles and unpair the generic things from their specific implementations.\nAnd do not forget, \u0026ldquo;never say never\u0026rdquo; (we all know the customer\u0026rsquo;s \u0026ldquo;it will never happen!\u0026rdquo;).\nSummary All the rules are trying to focus on the application level and dependency connected decisions. But in reality, you can apply them even on lower levels like objects or functions etc. You can try to specify your own questions which will help you to decide consistently in repeating situations. Creating such a questions/rules will help whole team to be in line with the goal and the architecture will by more consistent even when architect will be someone else.\nAll the rules are helping even with implementing all the SOLID principles, because they are trying to guide architect to define clear scopes and helps with separation of concerns (and connected SOLID principles). Do not forget, that architect doesn\u0026rsquo;t need to be developer and doesn\u0026rsquo;t need to understand technical language used by devs. Writing the rules in \u0026ldquo;common language\u0026rdquo; makes them understandable for everyone in the team. Naming all the things can help with the discussion about the architecture when needed.\nDo not hesitate to contact me if you have some questions or comments!\n",
    "ref": "/posts/sali_part2/"
  },{
    "title": "SaLi architecture for Microsoft Dynamics 365 Business Central, part 1",
    "date": "",
    "description": "",
    "body": "If you are developing solutions for Microsoft Dynamics 365 Business Central and you are building them from multiple applications (see Split or not to split), may be you are now experiencing some problems with the dependencies between your apps, chaos made by development withouth rules. We were there too and this is example of dependency chart of one our solution: Dependencies before SaLi And because my coleague Jakub Linhart is smart and tried to analyze our existing solutions to find some ways how to make them and our new solutions better, we have noticed some patterns and we tried to make some rules based on these patterns. Then I read book \u0026ldquo;Clean Architecture\u0026rdquo; from Robert C. Martin and learned about SOLID principles. And all started to give some meaning. And SaLi was born.\nSaLi Why SaLi? Just simple because Sacek and Linhart. But WHY SaLi? Because doing good architecture without rules is like trying to build high tower from bricks just by throwing the bricks on one heap. Bricks on the heap And SaLi is giving us some rules we can follow, to create nice solution which will be SOLID. Bricks with some rules Application Groups First thing we have noticed was, that applications we have developed, could be divided into multiple groups based on their purpose. This groupping was already mentioned in Split or not to split article. But we made some fine-tuining on them and weare now using these application groups:\nShared/Discrete apps Apps providing discrete functionality or adding totally new area of functionality. These apps can live alone (or on top of \u0026ldquo;standard\u0026rdquo; functionality) and will provide functionality to the end-user which could be used directly (Discrete functionality). Another attribute of these app is, that they have data which are used by many other applications in the solution (Shared). In these apps you have mostly some Master tables, Journal tables, Ledger tables, Document tables etc.\nProcess/Automations apps Apps of this type are interconnecting other apps, adding some automation on top of existing functionality. They are mostly not adding new processes or functionality, they are simplifying existing functionality or are \u0026ldquo;glueing\u0026rdquo; functionality of multiple separate apps. They have no meaning withouth the apps which they are extending and are implementing different processes of the company.\nInput/Output apps These apps are defining outer interface of the system. Are implementing APIs (input/output for other systems), changing GUI (output to display), adding Reports (output to printer/display). They should not have functionality which is not directly connected with the interface they are connecting. They are interface-specific (implementing specific interface connection, API vendor etc.).\nLayers In next step, after defining application groups, we have found that these apps should create layers, which are defined by their dependencies. Together with SOLID principles, we can assign different attributes to the groups like:\nShared/Discrete apps are defining critical business processes, are more abstract/universal, are stable (not changed too often), depending on less apps, more apps are depending on them Process/Automation apps are defining processes, are not stable as Shared/Discrete apps, could depends on more apps than depending on them Input/Output apps are unstable because are changed often, are depending on specific interfaces (are specific), are depending on other apps, no apps are depending on them And this created layers which could look like this: SaLi layers as circle Dependencies Rule Based on SOLID principles, the dependencies could lead only from outer layer to inner layer, never oposite, because unstable apps should depends only on more stable apps. Specific apps should depends on generic apps, not critical apps should depends on more business critical apps etc.\nThis gives us first rule of SaLi: \u0026ldquo;Dependency direction is given by layers into which the apps belongs. Dependency could go only from outer layer to inner layer (or from top to bottom if, see later). If needed in oposite, use the Dependency Inversion Principle to switch the dependency direction\u0026rdquo;.\nAnother way how to describe the layers is like this: SaLi layers You can see that we have even the AppSource app layer and Core app layer. These are apps out of our control and we can only depend on them. But even when you are developing solution for AppSource, the layers Shared/Process/Input\u0026amp;Output are about your AppSource solution (if it is created from multiple apps) and the AppSource layer means other AppSource apps you can be depending on.\nThe arrows are marking the direction in which the dependencies could be created in between apps.\nApplying this rule, and use the layers when creating application dependency chart, will result in something like this: Dependencies after SaLi But still, on this picture you can see arrows which are breaking the rule (connecting e.g. apps on same layer, going from bottom up etc.). Yes, these are errors in the architecture, and SaLi is helping us to see them. When we see them, we can fix them by implementing Dependency Inversion Principle (using interfaces, events\u0026hellip;). But without SaLi they are nicely hidden and will pop up in the least appropriate time.\nAnother income of SaLi in this case is, that we have the layer assigned to the app stored in our metadata for every app (we are using Work Items in Azure DevOps for this) to be able to create the chart. And because this, we can implement check in DevOps pipelines which can validate if the dependencies are in correct directions and can enforce developers to create them correctly (work in progress on this).\nNext In next article about SaLi we will look on other rules we are using.\n",
    "ref": "/posts/sali_part1/"
  },{
    "title": "How to publish your local BC containers to internet using IIS",
    "date": "",
    "description": "",
    "body": "If you are doing Dynamics 365 Business Central development on your local (not Azure) containers, may be you want to have access to them from outside your local network without using VPN. And not only to BC Web Client, but even to API/OData and development endpoint. And of course you want to have them published with some trusted certificate to be able to use all the functionality like Business Central application, connection from Power BI connectors etc. It is handy when you are working with external workers like freelancers, because you do not need to setup VPN etc. for them. When using AAD authentication, just invite them as guest users to your tenant and you are ready to give them access as needed.\nDifferent tools exists for solving that like Traefik.\nBut if you want just easy solution using Windows IIS, you can use the Application Request Routing (ARR) feature of IIS for that.\nHow it works When we create the proxy server (server with Windows and IIS installed) and we put it somewhere into our DMZ (zone which is accessible from internet and can access local network), we can setup DNS server to send our requests for some specified subdomain with our \u0026ldquo;BC Environments\u0026rdquo; to that proxy. The proxy takes the request and will forward it to correct internal server/container and will transfer the response back to the sender. In my case, all our internal development environments are running as part of our dev. subdomain, thus I am able to forward everything going to xxx.dev.ourdomain.com to our local xxx.dev.ourdomain.com.\nProxy architecture And if I set the proxy to listen on port 443, 7047, 7048, 7049 and 7085, I will make the web client, SOAP, OData/API, Development and Snapshot debugging available for people from outside.\nNeeded components External DNS Entry On your external DNS for your dev.ourdomain.com zone, you need to create wildcard entry of type CNAME leading to the external interface of your PROXY. It will translate anything in format xxx.dev.ourdomain.com to the proxy server address.\nInternal DNS Entries On your internal DNS you need to translate same address as you want to use from outside, to local address of the server/container. It means, on your internal network the address xxx.dev.ourdomain.com should be translated to the local address of the server/container. In this way we do not need any \u0026ldquo;translation\u0026rdquo; table which will transform the external name to internal one and we can just maintain the local DNS entries. Having same external and internal address is even good thing when you want to use AAD authentication in BC.\nForward Proxy On the proxy, install IIS feature with ARR and URL Rewrite. You can use this documentation to install and setup what is needed. Enable the SSL offloading to be able to have different internal and external SSL certificate. When the offloading is enabled, the proxy will act as man-in-the-middle. It means the external client is communicating with the external https endpoint of the proxy, which have some externally trusted certificate installed on it. The proxy then takes the request and will create new connection to the local resource and send the request there. The local resource could have internally trusted certificate, or could use http protocol without certificate if you want.\nSet the rules like this:\nGlobal forward rule Proxy rule 1 Rule to handle OAuth request correctly (without this the OAuth will end on the container instead the Microsoft site) Proxy rule 2 Rule to handle OAuth request correctly (without this the OAuth will end on the container instead the Microsoft site) Proxy rule 3 Set the default web site on the IIS to listen on the required ports like 443, 7048, 7049 and 7085. Open these ports for access from internet on the firewall.\nConclusion If everything is working correctly, you should be able:\nconnect to the xxx.ourdomain.com web client and other endpoints from local network connect to the xxx.ourdomain.com web client and other endpoints from internet without using VPN have trusted certificate placed only on the proxy server on the default site have internal certificate on your environments (proxy server must trust them) If you are missing some info here, let me know and I will update the article as needed.\nWarning! This setup is forwarding everything going to the *.dev.ourdomain.com to the proxy, thus it could be used for attack to your internal infrastructure, if you have more services available than just the BC containers on your internal dev.ourdomain.com domain. You need to be aware of that and you can somehow limit this by modifying the rules e.g. to forward only request having format of the names of your BC environments etc. Best is to use subdomain where is nothing else than the environments.\nEnjoy work with the environments withouth VPN!\n",
    "ref": "/posts/proxy/"
  },{
    "title": "Using BCLicense file from powershell",
    "date": "",
    "description": "",
    "body": "New License format .bclicense If you are working with OnPrem customers having Microsoft Dynamics 365 Business Central, you have already noticed the new license file format .bclicense. It was created by Microsoft to have modern way how to pass the licensing info without limits of the old .flf format (limited max size etc.). It had some side-effect (bugs) on beginning but I hope that they are solved (never tested yet). But today I am not writing about this file format because Customers, but because it could be handy for Partners/Developers and their CI/CD pipelines.\nWhy developers should look at .bclicense file Because you need to test that all your objects in your app are assigned in the Customer\u0026rsquo;s license. Because world is not ideal, still there are plenty of customers using OnPrem for running their BC environments. And it means you need to assign object IDs into the license when you are developing some PTE. Forgetting to do so is one of the top reasons of problems during or after releasing new version of PTE into customer\u0026rsquo;s environment.\nOne way is to use the customer\u0026rsquo;s license during running your Automated Tests. You can do that easily by importing the customers license into your container/environment and run the tests. But if you want to fail fast if something is missing, and you want to be sure that everything is ok, it will not be enough. Waiting for the container to be created and the test finish will take time, and if there is not enough code covered by tests, you can still have problems. But it is still good to use the customer\u0026rsquo;s license for the tests, because you can catch the missing object permissions e.g. when doing something with tables which are limited in access by default (ledger entries etc.).\nHow to use .bclicense to check permissions If you take look at .bclicense file, you will see that it have text header as old .flf file, and rest is XML. We are interested in the XML itself, because it keeps all the info we need to work with. If we will be able to read it and work with it as with xml document, we can go through the permissions and compare them e.g. with objects we have in our AL app. But how we can access the xml part of the file in powershell? It is not so hard. Just use this PowerShell script:\n$filecontent = get-content -Path my.bclicense [xml]$xml=($filecontent[$filecontent.count-1]).Remove(0,2) After that, we have the license as XML in our $xml variable. The expression will take last line from the file (the whole XML is one-liner at the end) and remove the beginning two characters (2x U+feff).\nAnd now you can do just the standard XML tricks with the variable to check what you want. The structure looks like this:\n\u0026lt;License\u0026gt; \u0026lt;Properties\u0026gt; \u0026lt;Property name=\u0026#34;xxx\u0026#34; type=\u0026#34;yyy\u0026#34; id=\u0026#34;nnn\u0026#34;\u0026gt;\u0026lt;![CDATA[datadatadata]]\u0026gt;\u0026lt;/Property\u0026gt; ... \u0026lt;/Properties\u0026gt; \u0026lt;PermissionCollections\u0026gt; \u0026lt;pl t=\u0026#34;ObjectType\u0026#34; c=\u0026#34;count\u0026#34;\u0026gt; \u0026lt;ps\u0026gt; \u0026lt;p f=\u0026#34;from\u0026#34; t=\u0026#34;to\u0026#34; pbm=\u0026#34;PermissionMask\u0026#34; /\u0026gt; ... \u0026lt;/ps\u0026gt; \u0026lt;/pl\u0026gt;\u0026gt; ... \u0026lt;/PermissionCollections\u0026gt; \u0026lt;Signature\u0026gt; ...digital signature of the file... \u0026lt;/Signature\u0026gt; \u0026lt;/License\u0026gt; As you can see, it is simple XML. We are interested in the node /License/PermissionCollections. There we can find node for each license Object Type (TableDescription, TableData, System, MenuSuite, Codeunit, Page, FieldNo, Report, Query, XMLPort, Dataport, Form, LimitedUsageTable). In the ps (permission set?) node we can find the p node (permission?) for all the ranges in your license. It includes even ranges which you do not have in your license assigned, because such a ranges could have still some permissions in the license assigned, like \u0026ldquo;-MD-\u0026rdquo; permissions to be able to remove old unnecessary objects.\nPermissionMask value meaning Based on analysis of the file I found this bitmap mask for the PermissionMask value:\nd mirX DMIR Lowest bit of the value is on the right. It means the R permission is assigned if the value is odd and is not assigned if value is even.\nExamples:\nd mirX DMIR\r------------\r0 0000 1000 - ---D- - 8\r0 0000 1100 - --MD- - 12\r0 0000 1111 - RIMD- - 15\r0 0001 0000 - ----X - 16\r0 0001 0001 - R---X - 17\r0 0001 0010 - -I--X - 18\r0 0001 1000 - ---DX - 24\r0 0001 1001 - R--DX - 25\r0 0001 1010 - -I-DX - 26\r0 0001 1101 - R-MDX - 29\r0 0001 1111 - RIMDX - 31\r0 1100 0001 - Rim-- - 193\r0 1100 1001 - RimD- - 201\r1 0100 0101 - RiMd- - 325\r1 1000 0011 - RImd- - 387\r1 1100 0001 - Rimd- - 449\r1 1110 0000 - rimd- - 480 Thus you need to be aware that it is not enough to test if the ID of your object is included in some range, but you need to check if there is X permission to execute it on the range, M and I permission to be able to modify or create the object by publishing standard APP package (if these permissions are missing, you can only publish such an object by Runtime Package) etc.\nYou can test the permission e.g. by this function:\nfunction Check-PermissionMask { param( [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true, HelpMessage=\u0026#34;Permission value\u0026#34;)] [int]$PermValue, [Parameter(Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true, HelpMessage=\u0026#34;Permission to test\u0026#34;)] [validateset(\u0026#39;r\u0026#39;,\u0026#39;m\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;X\u0026#39;,\u0026#39;R\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;I\u0026#39;,\u0026#39;D\u0026#39;)] [string]$PermToTest ) $PermMask = \u0026#39;RIMDXrimd\u0026#39; $TestValue = 1 -shl $PermMask.IndexOf($PermToTest) Return [bool]($PermValue -band $TestValue) } # Test of the function: Check-PermissionMask -PermValue 15 -PermToTest \u0026#39;R\u0026#39; True CI/CD process Thanks to the .bclicense you can extend your pipelines to test if license was correctly extended with new objects and you do not need to wait for the container (and fail as early as possible). You still need to solve one thing which is out of scope of this post - how to get the list of objects in your app without creating the container and publishing the app first. I am sure there will be some way (like going through all .al files and do some basic text parsing to find all the object headers, but it still could be complex e.g. to cover correctly temporary tables etc.). If you have some tip regarding that, do not hesitate to share it in comments. If I will create something regarding that, I will try to share it.\nSummary You can use the new .bclicense format to work with the data inside the license easily. But you need to solve few things first, like where the actual license for the customer will be stored, how to get the list of objects in app etc. Use the knowledge as you need, I hope that this article will save you some time you will need to spend on analysis of the file yourself. I am sure that there will be plenty things you can thing out around this.\nP.S.: Using unsupported DLL included in BC installation After publishing this article, I got info from Steffen Balslev from Microsoft (Thanks!), that there is DLL for handling the .bclicense file I can use to read the info. Of course, this dll and using it in this way is unsupported and undocumented, it means you are using it on your own responsibility. In reality, there are two DLLs - one for .netstandard (Microsoft.Dynamics.BusinessCentral.Bcl.dll) and one for .net framework (Microsoft.Dynamics.BusinessCentral.BclFwk.dll). I succeeded to use the .net framework version from PowerShell to handle the license file. The .netstandard one is missing some reference (but it could be my fault).\nHere is my PowerShell code which loads the dll for you:\n#Download latest w1 artifact $ArtifactPath = Download-Artifacts -artifactUrl (Get-BCArtifactUrl -type OnPrem -country w1 -select Latest) -includePlatform #Find the dll $DLLPath = Get-ChildItem -Path $ArtifactPath[1] -Filter \u0026#39;Microsoft.Dynamics.BusinessCentral.BclFwk.dll\u0026#39; -Recurse -File #region Load the assembly $MSAzureKeyVaultPath = Get-ChildItem -Path $ArtifactPath[1] -Filter \u0026#39;Microsoft.Azure.KeyVault.dll\u0026#39; -Recurse -File |Select-Object -First 1 $NewtonsoftJsonPath = Get-ChildItem -Path $ArtifactPath[1] -Filter \u0026#39;Newtonsoft.Json.dll\u0026#39; -Recurse -File |Select-Object -First 1 $MSAzureKeyVault = [Reflection.Assembly]::LoadFile($MSAzureKeyVaultPath.FullName) $NewtonsoftJson = [Reflection.Assembly]::LoadFile($NewtonsoftJsonPath.FullName) #Some magic to resolve dependencies $OnAssemblyResolve = [System.ResolveEventHandler] { param($sender, $e) foreach($a in [System.AppDomain]::CurrentDomain.GetAssemblies()) { if ($a.FullName -eq $e.Name) { return $a } } if ($e.Name -like \u0026#39;Microsoft.Azure.KeyVault*\u0026#39;) {return $MSAzureKeyVault} if ($e.Name -like \u0026#39;Newtonsoft.Json*\u0026#39;) {return $NewtonsoftJson} return $null } [System.AppDomain]::CurrentDomain.add_AssemblyResolve($OnAssemblyResolve) try { Write-Host \u0026#34;Loading assembly\u0026#34; Add-Type -Path $DLLPath.FullName -Verbose }catch { Write-Host \u0026#34;Exception:\u0026#34; -ForegroundColor Red $_.Exception.LoaderExceptions } [System.AppDomain]::CurrentDomain.remove_AssemblyResolve($OnAssemblyResolve) #endregion The final use of the loaded assembly could be like this:\n$LicenseFile = \u0026#39;my.bclicense\u0026#39; $stream = [System.IO.StreamReader]::new($LicenseFile) $reader = [Microsoft.Dynamics.BusinessCentral.License.BcLicense.LicenseReader]::new($stream.BaseStream) $reader.GetObjectRangePermission(\u0026#34;Page\u0026#34;,18) Output will be like this:\nRangeStart : 4 RangeEnd : 56 Read : Direct Insert : None Modify : Direct Delete : Direct Execute : Direct Expiry : In this way, you do not need to parse the values etc. Everything is done by the class created for this purpose. But you are not in charge of the code. Choose wisely!\n",
    "ref": "/posts/bclicensestructure/"
  },{
    "title": "How to translate your AL app from Devglish to Endglish",
    "date": "",
    "description": "",
    "body": "Since we started to use VSCode for AL development, I hear discussions that writing UI texts like ToolTips etc. should be done by someone else than developer. And I must agree. We - developers - are lazy kind and writing text is not our popular game. We are able to write some generic text, mostly automagically generated by different tools we are using to write code, but such a text is mostly equal to 0 Shannons (no, I am not talking about @Shannon Mullins). But how to solve this?\nText written by developer Ask Microsoft to solve it On many occasions was this subject mentioned and discussed with Microsoft product group. I am sure that they are aware of this, but the list of To-Do is too long and nobody knows where in the list is this to solve. And I understand that, because there are things which are more important. But may be, once we will get something which will solve that.\nDo you know the \u0026ldquo;Blend for Visual Studio\u0026rdquo;? It is tool for designers to create the design of the app, without solving the code behind. Visual Studio is used for writing the code, and let the design be done in the Blend. Of course, we do not need such a huge tool.\nAnother example how these things are solved in other applications are the \u0026ldquo;placeholders\u0026rdquo; (X++ in Dynamics 365 AX was using this if I remember correctly and I assume that the successors are still using it). Developer just use some kind of code instead text and the text is assigned to the code somewhere outside the code. But it is much harder to understand the code, when you see only something like Message(\u0026quot;@SYS654433\u0026quot;).\nUse what we have But today I had discussion with my colleague Martin, and we were thinking how to put our consultants in charge of the content of our ToolTips etc. Of course, one way was to give them access to the code and let them overwrite the texts in the code. And I do not like it (of course, they will need e.g. to have license for Azure DevOps to be able to work with the repos etc.). But then I realized, that there is already one way, which our consultants are using to manage the texts - translations. They have possibility to do the translations to different languages as needed, because the XLIFF files for our apps are available and could be updated and then developers can incorporate the translations into new versions of the apps. It means, we already have everything what we need.\nHow? Just create new translation file for en-us language (there are multiple vscode extensions which could help you with the xliff files management - e.g. XLIFF Sync or NAB AL Tool) and do the ENU to ENU translation. This time it will not be about translating the text as it is to different language, but rather create new, better text describing all what end-user needs to see. In this way, developers could put \u0026ldquo;placeholders\u0026rdquo; into the code, e.g. just simple description what is going on, which text it is etc. (like \u0026ldquo;Tooltip for field My New Shiny Value\u0026rdquo;) or just use the generic values as you are used to. No problem. And then, let the consultants translate these \u0026ldquo;technical\u0026rdquo; texts into the real helpful end-user-centric texts. I am sure, that you already have processes which covers translation of your apps. And if something will not be \u0026ldquo;translated\u0026rdquo;, users will see the standard text from developers. And consultant could search for this text in the en-us translation file and change it as needed, you do not need to fix that in code (if it is not issue of fact etc.). Just update the translations, publish new app version and you are done!\nConclusion I see as best practice to have en-us XLIFF file in each app to solve this translation from \u0026ldquo;Devglish\u0026rdquo; (Developer\u0026rsquo;s English) to \u0026ldquo;Endglish\u0026rdquo; (End-user English). Let developers write the text in code in Devglish, and let consultants add the Shannons to the texts to transfer information to the Users and create the new Endglish layer.\nTranslation from Devglish to Endglish Downside There is one downside of this: your translation sources for other languages will be based on the Devglish version, not the Endglish version of the EN texts. But it is solvable and we will see, if some vscode extension will add functionality allowing to update the translation files from .en-US. version instead .g. version of the xliff. It means having the update chain changed from\nflowchart LR\r.g. --\u003e .cs-CZ.\r.g. --\u003e .sk-SK.\rto this flowchart LR\r.g. --\u003e .en-US.\r.en-US. --\u003e .cs-CZ.\r.en-US. --\u003e .sk-SK.\rIt will allow us even to update the translation in other languages when someone change the Endglish version and thus keep in sync the languages. I believe in BC community in this!\nP.S.: Do not put the Endglish translations into the .g. file directly, they will be lost when you compile the app, because .g. file is generated file, it is not source file. This is why it should not be part of the source code repository (add it into .gitignore file).\nFeedback If you have anything to add, comment or you want to share your way of solving these things, do not hesitate and put your comments here! I am looking forward the feedback and I am open to discuss this!\nThanks!\n",
    "ref": "/posts/whytowriteenutranslation/"
  },{
    "title": "Unattended access to local resources - WCF Relay",
    "date": "",
    "description": "",
    "body": "In many cases you need to work with resources (files, scales, card readers etc.), which are available on local network/PC, from within Microsoft Dynamics 365 Business Central which runs somewhere in the Cloud. I will show you one way how to solve these things.\nWhat is the problem? Different processes in Business Central can need some local resources of yours. In a simples case it could be some file on your disk on your PC (or server on local network). Standard way is, that BC client will ask user to select the file which should be uploaded or select where to save downloaded file. It needs user interaction. But many automatic processes needs to run without this user interaction. E.g. to monitor some local folder for new files, read them, process them and move them to archive. You can use some Azure Storage to be able to do that in compatible way, but in some cases it is not possible, for example when the file is generated by some ancient technology or some local hardware. Still you have possibility to \u0026ldquo;synchronize\u0026rdquo; these local files to the cloud storage by some technology, but it have own problems (we were using OneDrive synchronization for this, but the reliability was very low, mainly because we needed pseudo-online updates).\nHybrid solution To solve this problem, we can use some local running component, which have access to the local resource, and somehow connect it with the Cloud. \u0026ldquo;Standard\u0026rdquo; solution will be to have service, which offer some API. But because you need to connect from Cloud (from internet), you will need to somehow tunnel the outside incoming traffic to the local service. It will mean to create \u0026ldquo;hole\u0026rdquo; in your firewall to allow this. To limit possible security issues you can try to limit this hole for specific IPs from which the traffic will be allowed, but it is hard to do that for Business Central, because I do not know any specific IPs which are used for the BC services. Hybrid connection diagram Luckily we have different Azure Services, and one of them is solving this issue - Service Bus (specifically WCF Relay). This service is allowing your service to work in \u0026ldquo;opposite\u0026rdquo; direction - your service is connecting to the cloud from inside your network. It means you only need to allow outgoing connection in your local network which is much easier (and mostly is not limited, because it is standard connection to https endpoint on internet). Azure Relay will create the public endpoint on which you can consume the API you defined in your locally running service. To implement this you need only few settings in the app.\nOf course, it have some cost. You are paying some small amounts per each \u0026ldquo;relay hour\u0026rdquo; - it means for each hour your service is running and is connected to the Azure Relay (I can see standard cost 0.0090 EUR per 100 relay hours and 0.009 EUR per 10 000 messages).\nImplementation To implement this solution, you need to follow these steps:\nImplement Windows Service with Rest API with local endpoint Implement WCF Relay to have public endpoint Implement BC connector for this service Windows Service with Rest API How it could look like you can see here: LocalFS Service\nI am not expert in c#, thus take it as a work of beginner based on examples found on internet (like this).\nThis windows service will open one local endpoint on port selected in the config. Second endpoint will be created through Azure WCF Relay.\nService have 3 main areas:\nREST API (contract and implementation of the API) Windows service handling (main program and actions to register/start/stop the service) Using the WCF Relay to create public endpoint WCF Relay To be able to connect to the locally running service from outside your local network, you need to connect it to Azure Service Bus. To be able to do this, you need to create the Relay namespace in Azure portal. Creating Relay namespace The namespace is creating the URL which is unique for your relay. The public endpoint you will need to connect to, will have name like this:\nhttps://{RelayNameSpace}.servicebus.windows.net/{RelayServicePrefix}/{something}\nWCF Relays could be of two types: Dynamics or Static. Static can be created on the portal, but we will rather use the dynamic ones. They are created and removed automatically when you start/stop your service. RelayServicePrefix in the URL is any string you will choose. In this way you can group multiple different services together. The last part of the URL is again text you choose to make the URL unique. In my example code I am using computer name and domain name to create this part (like \u0026ldquo;PC0123.mydomain.local or \u0026ldquo;PC0123.\u0026rdquo; if there is no domain).\nTo be able to register the service into the relay, you need to use Shared access policy key. You can get it on the Azure portal in the Shared access policies section. Getting SAS key When you set the values and run the service, you should see the service in the WCF Relay list on the Azure portal.\nBusiness Central connector To connect to the service from Business Central, use standard HTTPClient to connect to the relay address. It is standard REST API, thus it should not be a problem for you to do that. If you want to use this LocalFS service, you can use our AppSource app Navertica Local FS Connector.\nI am sure that you will find many examples how to implement REST API client in AL on internet.\nSecurity The WCF Relay is transparent and it means it is on your service to make the authentication of the caller. In the example I am using just basic mechanism which checks that the used user name and password is same as configured one.\nI am sure that there is some possibility to add security on the WCF Relay, but have not time to dig into it yet. If you have some tips, you can use the comments to share them.\nConclusion I wanted to show you possible way how to handle things which were blockers some time ago for using Business Central Online. You can easily modify the service to not handle local files access but e.g. communicate with some locally connected hardware (special printers, scanners, scales etc.). Such a things should not prevent you now from going to cloud. It have some cost (cost of the WCF Relay), but I think it is really small cost which is overweighed by what you will get.\nI know that there are other ways how you can solve such a cases, but this one is for me easy to use even with my limited knowledge. I hope it will help you to think outside the box and unblock your way.\nSee you in the Cloud!\n",
    "ref": "/posts/localfs/"
  },{
    "title": "Snapshot debugger on OnPrem - what you need to know to set it up",
    "date": "",
    "description": "",
    "body": "When you want to use Snapshot Debugging on your OnPrem environment, may be you can hit this error when trying to initialize it:\nError: The SSL connection could not be established, see inner exception.\r... In some cirumstances you can even have problem to start the BC Service when you enable the Snapshot Debugger endpoint.\nWhat is the problem? For standard endpoint, which we are using for longer time already, like client endpoint (default port 7046), OData (7048) and Dev (7049), the Management console for BC Server is automatically setting some things in background when you save the settings in it. But for the Snapshot debugger, this is not done and you need to do it yourselfs.\nURLACL First thing is, that for the user account, under which is BC Server running, usage of the HTTP address is reserved (see this). Manually it could be done by running this command as admin on the server:\nnetsh http add urlacl url=https://+:7083/BC/ user=\u0026#39;NT AUTHORITY\\NETWORK SERVICE\u0026#39; You need to modify the URL if you are not using SSL (use http instead https) and port number/instance name. User needs to be the one under which the service is running.\nTo list existing urlacl you can use this command:\nnetsh http show urlacl If this is not done, you will ger some error about \u0026ldquo;unable to listen\u0026rdquo; on the specified address in your event log.\nSSLCERT Second step is to assign certificate to the address, if you are using SSL for the endpoint.\nTo use SSL it is not enough to set the certificate in the BC Service Management console (in the BC Service configuration), but the certificate must be assigned to the address.\nThis is done again through netsh command:\nnetsh http add sslcert certhash=\u0026lt;thumbprint\u0026gt; appid=\u0026#39;{00112233-4455-6677-8899-AABBCCDDEEFF}\u0026#39; ipport=0.0.0.0:7083 Use the thumbrint of the certificate you want to use (e.g. same like in the BC Service configuration), appid could be any GUID (e.g. like in th example), and ipport must be the port for which you are assigning the certificate.\nTo list existing assignments you can use this command:\nnetsh http show sslcert If you want to assign certificate to port, where already is certificate assigned (e.g. when renewing expiring certificate), you need to delete the sslcert first:\nnetsh http del sslcert ipport=0.0.0.0:7083 Some hicups Sometimes, when you are changing the SSL settings on the BC server, you can get into issues that when saving the configuration you get some errors that Certificate cannot be registered or something similar. It is mostly in situations, when the settings and the current state of the SSLCERT is not in line - it means, you are disabling SSL but SSLCERT is not registered (e.g. after you have copied configuration from another instance having SSL enabled and you current instance had it disabled) or vice versa. In this case you need to fix this discrepancy (when disabling SSL, create the SSLCERT assignment first, when enabling, remove the existing assignemnt first).\n",
    "ref": "/posts/snapshotdebuggeronprem/"
  },{
    "title": "Architecture of PTE - Split or not to Split?",
    "date": "",
    "description": "",
    "body": "On Directions EMEA in Milan I delivered session about Architecture of PTE for Business Central. I was trying to find some rules when or why you should or shouldn\u0026rsquo;t split your PTE into multiple apps. Now I will try to catch this into this article.\nSplit or not to split Some partners are putting everything into one app per customer. Some are splitting all to separate apps based on different rules like per process, per area etc. Our company is going through the way of splitting, rather than not splitting, because on the beginning we wanted to have possibility to re-use the \u0026ldquo;tiny apps\u0026rdquo; for others if needed. But the result? Here is the picture (taken from real live customer project):\nReal world apps dependencies example As you can see, some apps are \u0026ldquo;self standing\u0026rdquo;. This is what we wanted to have. But most of them are connected with others. That\u0026rsquo;s something we didn\u0026rsquo;t want. What does it mean? They are not re-usable, if you do not want to re-use even the dependencies. It means you can reuse only the \u0026ldquo;leftmost\u0026rdquo; apps. It makes most of the apps non-reusable without refactoring.\nBut the question stays: Split or not to split?\nTypes of PTE apps based on the chart, I made some analysis and found these sets of apps based on names of the apps:\nTOOLS/LIBRARIES - \u0026ldquo;Record links\u0026rdquo;, \u0026ldquo;Control Focus\u0026rdquo;, \u0026ldquo;Barcodes\u0026rdquo;, \u0026ldquo;PDF Signature\u0026rdquo; IMPORTS/EXPORTS - \u0026ldquo;Data Migration\u0026rdquo;, \u0026ldquo;Salary Import Connector\u0026rdquo;, \u0026ldquo;Banking\u0026rdquo; CONNECTORS - \u0026ldquo;Manufacturing API\u0026rdquo;, \u0026ldquo;Laboratory Integration\u0026rdquo;, \u0026ldquo;NAV Web Service Adapter\u0026rdquo; SIMPLE PROCESSES - \u0026ldquo;VAT Posting Date Change\u0026rdquo;, \u0026ldquo;Standard Cost Management\u0026rdquo;, \u0026ldquo;Approval\u0026rdquo;, \u0026ldquo;On Hold\u0026rdquo; COMPLEX PROCESSES OR AREAS - \u0026ldquo;Service Acceptance\u0026rdquo;, \u0026ldquo;Spare Parts\u0026rdquo;, \u0026ldquo;Overhead Material\u0026rdquo;, \u0026ldquo;FA Inventory\u0026rdquo;, \u0026ldquo;Price Security\u0026rdquo;, \u0026ldquo;Manufacturing\u0026rdquo;, \u0026ldquo;Report Pack\u0026rdquo;… EXTENSIONS (CUSTOMIZATIONS) - \u0026ldquo;Handling Unit Extension\u0026rdquo;, \u0026ldquo;Continia Extension\u0026rdquo;, \u0026ldquo;Small modifications\u0026rdquo;, „Intrastat Extension\u0026quot; We can define some properties of each type, which could help us to identify them:\nTOOLS/LIBRARIES - Adding functionality which is used through whole system, independent on specific processes/areas IMPORTS/EXPORTS - working with files or other data sources CONNECTORS - connecting with other systems (through API - as client or as server) or between extensions SIMPLE PROCESSES - mostly adding some small tasks and processes, limited schema changes (not many fields and tables) COMPLEX PROCESSES or AREAS - creating new processes or whole areas, new tables, pages… EXTENSIONS (CUSTOMIZATIONS) - are modifying app which is in responsibility of someone else (3rd part, different product group etc.) And based on their properties, we can divide them into two groups:\nNot Problematic TOOLS - mostly depending on standard only (lowest level dependencies) IMPORTS/EXPORTS - low probability of depending apps, rather depends on other (similar to CONNECTORS), sometimes need to be OnPrem if we are OnPrem CONNECTORS - should be top (or bottom) app in the tree Problematic SIMPLE PROCESSES - you do not know if it will not grow to be complex COMPLEX PROCESSES or AREAS - hard to split the processes correctly and solve the dependencies when cyclic EXTENSIONS (CUSTOMIZATIONS) - could have depending apps, could grow to complex processes But still: Split or not to split?\nDifferences Ok, previous classification is not helping us to answer the question. We should look at the CONs and PROs of the splitting the PTE into separate Apps:\nSplitting PRO: Easier parallel development (development split to more apps) Clear responsibilities Simpler/Smaller apps Simpler maintenance Re-use (really?) List of functionalities Possibility to „uninstall\u0026quot; customization when not needed (really?) CONS: Dependency „hell\u0026quot; (solvable) Architecture decisions are more complex Performance (solvable) Release complexity (solvable) Cost of maintenance per app (partially solvable) We can discuss if the apps are re-usable (we already touched this). But most of the CONs are solvable by using correct tools (partial records etc.) and automatic processes (release pipelines, powershell etc.).\nAll-in-one PRO: Simplified Architecture Simplified release to Customer Cost of maintenance per app Performance (! not because technical issues, but because mindset of the developers - \u0026ldquo;it is in one app, why bother with performance?\u0026rdquo;) CONS: Developers conflicts (solvable) Release Planning (solvable) Installation times (? in history, deploying big app took more time) Impossible to re-use without refactoring What is inside (solvable) Shared responsibilities And again, you can see that most of the CONs could be solved by tools (AL Object ID Ninja, documentation etc.) and processes (Azure DevOps planning etc.)\nStill: Split or not to split?\nComparing Ok, compare the PROs and CONs of both methods when we remove the solvable items:\nPROs and CONs comparison Items I think are most interesting are in bold.\nAll depends on priorities you have in your company or team. If your priority is e.g. \u0026ldquo;Simpler maintenance\u0026rdquo; and \u0026ldquo;Clear responsibility\u0026rdquo; and you sacrifice the \u0026ldquo;easy architecture decisions\u0026rdquo;, go and split the apps.\nIf your priority is simpler \u0026ldquo;architecture decisions\u0026rdquo; and you sacrifice \u0026ldquo;shared responsibility\u0026rdquo; (who is responsible for the content of big, all-in-one app?), go and do not split.\nOk, does it mean split or not to split?\nSMART Architecture For me, both ways are valid. But you need to do it SMART.\nSIMPLE MANAGABLE ARCHITECTURE REACHING TARGET And to do it, you need to know what the target is for you, your team, your company, your customers.\nWhat it could mean? For example this:\nKeep TOOL/LIBRARIES and CONNECTOR apps separate Use Dependency Inversion principles (interfaces etc.) where you want to \u0026ldquo;reverse\u0026rdquo; the dependency Try to keep the responsibilities clear. Boundary of App is boundary of responsibility. Public interface of App is contract between apps. When you need to depend on new App (3rd party etc.), try to create CONNECTOR as separate app to prevent adding the dependency into the \u0026ldquo;core\u0026rdquo; app if possible. When you need to switch the app to „OnPrem\u0026quot;, create CONNECTOR (working with local files etc.) - leave rest of the logic in „Cloud\u0026quot; (and best is to not use OnPrem at all - it will cost you, literally) Content of the app must be in line with app name. If the change is not in line with the app name, may be it should be somewhere else. Choose wise the name of the app. Conclusion May be you have still questions, what is THE BEST way. But I think there is no silver bullet for this. The answer could be different, depending on who you ask. But if you already go some way, do not switch just because someone tells you that it is better in another way. May be he/she is solving different problems than you and your solution in your situation is the best for you.\nI hope that this article helps someone to think about the architecture of Business Central PTEs from different angles, and I am open to discussion about the architecture at all. What I see now as biggest gap in our community, is lack of architects and discussion about the architecture itself. And this my session and article is for me starter for the discussion we will have next years.\nBe safe! See you online/onprem!\n",
    "ref": "/posts/ptearchitecture1/"
  },{
    "title": "My new blog",
    "date": "",
    "description": "",
    "body": "New blog May be you already noticed that my original blog on dynamicsusers.net (original URL was https://www.dynamicsuser.net/nav/b/kine) is gone. I am building new one from scratch. May be it is time to begin something new and cut off the Dynamics NAV history.\nBecause todays it is easy to use GitHub as a platform for the blog (and thus using git and vscode to \u0026ldquo;post\u0026rdquo; the articles), I decided to use HUGO as the framework. Another side of this is, that my blog is \u0026ldquo;opensourced\u0026rdquo;, thus you can see everything what is there and if you have some need to fix something, you can easilly post pullrequest.\nI hope that you will find it worth to read it.\nEnjoy it!\n",
    "ref": "/posts/newblog/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "About me I have nearly 40 years of experience with software development and IT in general. In the year 2001, after finishing university, I joined the NAV world in NAVERTICA company as NAV developer. Having a wide area of knowledge in programming languages and connected areas is helping me to understand “how it works” and is giving me background for solving different tasks. For the last years, I am working more with GIT, Powershell, and Azure DevOps. I am a Microsoft Most Valuable Professional (MVP) since the year 2005.\nWhat I know I have learned mostly on my own (many trials\u0026amp;errors), because I started when there was no internet and even no books about computers. Our country lived in communism and my first computer Atari 800XE was bought in Austria (thanks to my parents for this decision). But all that gave me something, what cannot be taught.\nIn personal life I have beloved wife and my free time activity is to play PC games/simulators like IL-2 Sturmovik, MS Flight Simulator and other.\n",
    "ref": "/about/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "If you need professional services from me, contact NAVERTICA a.s. sales. I am employee in the company and I am not able to serve you directly.\nIf you want to support me personally, you can use the BuyMeCoffee tipping system.\n",
    "ref": "/contact/"
  }]
